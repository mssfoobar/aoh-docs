{"searchDocs":[{"title":"Overview","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/overview","content":"Overview This describes how to setup the infrastructure of a new project based on AOH. It consist of the following steps: Platform Infrastructure tasks (on-cloud / on-prem)AOH tasks","keywords":"","version":"Next"},{"title":"Configure AOH Applications","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/platform-aoh/configure-aoh-applications","content":"Configure AOH Applications Deplending on your projects, you can customise AOH applications' ports and endpoints according to your requirements.","keywords":"","version":"Next"},{"title":"Maintenance","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/maintenance","content":"Maintenance This section includes steps for maintenance and subsequent upgrades. AOH Database​ Pre-deployment​ Migration steps required for upgrade are identified. Identified migration steps are extracted. Deployment​ Identified migration steps are put into MINIO. Execute Migration Job to apply migration steps from MINIO.","keywords":"","version":"Next"},{"title":"Overview","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/platform-aoh/overview","content":"Overview //TODO","keywords":"","version":"Next"},{"title":"prepare-aoh-setup","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/platform-aoh/prepare-aoh-setup","content":"prepare-aoh-setup Ingress​ # Deploy traefik (from init folder) kubectl create ns traefik helm repo add traefik https://traefik.github.io/charts cd \\ar2-infra\\argocd\\&lt;yourcluster&gt;\\init\\traefik\\ helm install traefik traefik/traefik -f ./values-xxx-x.yml –namespace traefik Prepare DB DB Server should have already been deployed in previous steps found in Infra platform. Here we are installing DB schemas and essential data for AOH to function properly. Install Database​ Check with deployment team for the latest DB package to deploy. Apply overall Hasura Schema​ The latest DB package will also deploy the latest Hasura Schema","keywords":"","version":"Next"},{"title":"Setup Platform infrastructure","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/platform-infra/AWS-EKS/setup-platform-infra","content":"Setup Platform infrastructure AWS​ //TODO: include customization to terraform files Change directory to:/ar2-infra/terraform/terraform-aws-eks-blueprintes-v4/&lt;clustername&gt; Execute terraform apply. Wait for 40 minutes AWS - Database (If required)​ //TODO: add steps for RDS/Aurora/Serverless setup","keywords":"","version":"Next"},{"title":"Setup AOH Applications","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/platform-aoh/setup-aoh-applications","content":"Setup AOH Applications This section covers the setup of AOH. It starts off with ArgoCD as it is used as the main CD mechanism to deploy entire AOH. ArgoCD deploys: Foundation ServicesApplication Services Install Foundation Services​ Install ArgoCD​ # Create namespace kubectl create namespace argocd # Deploy kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml # Get ArgoCD access credentials kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath=&quot;{.data.password}&quot; | base64 -d; echo # Forward UI for access kubectl port-forward --address localhost -n argocd svc/argocd-server 19080:80 Add the necessary repo into ArgoCD​ argocd repo add https://github.com/mssfoobar/&lt;repo&gt; --username &lt;username&gt; --password &lt;git_key&gt; --insecure-skip-server-verification Create aoh storage class​ cd init/k8s kubectl apply -f sc—retain.yml cd /ar2-infra/argocd/&lt;clustername&gt;/ # Add root and project manifests (triggers sync) kubectl apply -f root-app-&lt;clustername&gt;.yml kubectl apply -f projects/project-&lt;clustername&gt;.yml Init (mass)​ folder: init/ # Init general secrets kubectl apply -f external_secrets/secrets # Init minio secrets kubectl apply -f minio # Deploy Traefik kubectl create ns traefik helm repo add traefik https://traefik.github.io/charts helm install traefik traefik/traefik -f traefik/values-xxx-x.yml –namespace traefik Prepare Keycloak​ //NOTE: requires MINIO, AWS secrets, Keycloak initialisation Put stengg.agiirad.keycloak.user.*#*#*#*#*.jar file into minio bucket common-iam/publicUpload it within bucket of common-iam while creating a folder named public Log into keycloak To be regen in keycloak and update into aws secrets: In Keycload, go to Clients, For Aoh_rnr: Place into aws_secrets: .common-rnr.iam.client_secret : &lt;secrets&gt; For Ar2_web: Place into aws_secrets: Select realms -&gt; ar2-&gt; Clients -&gt;xxxx service -&gt; Credentials Generate the Client secret and place them into the aws secrets under “iam”​ json file (re-11clean.json) to be imported from keycloak console Under keycloak local host UI, on top right left click drop down and Create Realm . Browse and upload the extracted version of the json file. Relam name should be auto-filled as shown below: Change Client Access setting in Keycloak,​ Go to Keycloak -&gt; Client -&gt; Client details -&gt; Access setting -&gt; valid redirect URIs Check Hasura system endpoints (should already deployed when DB is deployed)​ go hasura system -&gt; endpoint Reload Hasura metadata​ hasura --skip-update-check metadata --endpoint &lt;http://localhost:port&gt; --admin-secret &lt;hasura-admin-secret&gt; reload ","keywords":"","version":"Next"},{"title":"Configure Platform Services","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/platform-infra/AWS-EKS/configure-platform-services","content":"Configure Platform Services AWS​ // TODO: Secrets manager​ // TODO: Set up secrets manager outline to be filled in ACM​ Certificate ARN to be used by Ingress configuration. To be customised by project/deplpyment","keywords":"","version":"Next"},{"title":"Setup Platform infrastructure","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/platform-infra/K3S/setup-platform-infra","content":"Setup Platform infrastructure K3S​ //TODO: include customization to scripts/terraform files In-cluster Database (If required)​ //TODO: Could be done as part of AOH setup.","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/platform-infra/K3S/intro","content":"Introduction //TODO","keywords":"","version":"Next"},{"title":"Pre-requisites","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/pre-requisites","content":"Pre-requisites // TODO: Create infra repo template (based on ar2-infra) Firewall requirement: Name inbound/outbound Port type APPS/Web (inbound) 443 TCP APPS/Web/cloudwatch (outbound) 443 TCP APPS (outbound) 3478 UDP APPS (outbound) 5000 UDP APPS (outbound) 3478 TCP Cloudwatch (outbound) 443 TCP Stun server (inbound) 3478 UDP SFU TCP (inbound) 3478 TCP SFU UDP (inbound) 5000 UDP Database (outbound) 5432 TCP Software for installation client:​ Aws CLITerraformKubectlHelm Configuration​ AWS account and profile To set profile: For windows: setx AWS_PROFILE &lt;clustername&gt; aws eks update-kubeconfig --region ap-southeast-1 --name &lt;clustername&gt; For linux: export AWS_PROFILE=&lt;clustername&gt; aws eks update-kubeconfig --region ap-southeast-1 --name &lt;clustername&gt; ","keywords":"","version":"Next"},{"title":"Wrap up","type":0,"sectionRef":"#","url":"/aoh-docs/docs/deployment/wrapup","content":"Wrap up Point DNS (Route53) to Ingress Controller​ //TODO: To automate: fill up entries in Route53 for hosted zone Update load balancer with TLS secrets​ update and apply \\ar2-infra\\argocd\\&lt;your cluster&gt;\\init\\traefik\\values-for-aws-wfm.yml Route53 zones​ Setup set of shared dns names for the current version of deployment go to /ar2-infra/argocd/&lt;clustername&gt;/init/route53Replace the following with values from your cluster/environment: &lt;clustername&gt;&lt;hostedzoneid&gt;&lt;dns_name&gt; excecute the following command to get aws to generate the current route 53 records: aws route53 change-resource-record-sets --hosted-zone-id &lt;hosted-zone-id&gt; --change-batch file://&lt;route53_record_file&gt;.json NOTE: the easiest way to obtain the &quot;hostedzoneid&quot; is to go to the hosted zone alias create record page, Select Route traffic to &quot; Alias to Network Load Balancer&quot;, &lt;your region&gt;, &lt;your load balancer&gt;, and the page will show the &quot;Alias hosted zone ID&quot;, which is the &lt;hostedzoneid&gt; you want to input.","keywords":"","version":"Next"},{"title":"Introduction","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/devops_onboard/intro","content":"Introduction This section provides a detailed description of how services are initially setup and configured for CI/CD in AGIL Ops Hub. Setup Service CI pipeline​ The outputs of the CI pipeline that facilitates CD are: building and publishing of the container image to the container registryupdating the service's infra repository on the updated container image The .github/workflows folder in the service source-code repository (created from the template) contains GitHub actions to produce these outputs. It is configured to : build your service into a container image and publish this container image. The workflow currently publishes to ghcr.io (GitHub container registry).update the Service Infra Repository (to be setup in the following step) with the new container image. note This is for the initial setup only. Further customizations, such as the introduction of additional parameters for the service application could require modifications to this github workflow.","keywords":"","version":"Next"},{"title":"Maintenance","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/devops_maintenance/maintenance","content":"Maintenance caution If the service infra repository is referenced from the Project IaC Repo, changes made in the service infra repository may impact deployment. Based on updates to the docker-compose file in the Servers (both backend and frontend) Common updates to applications​ Addition of Env params​ Addition of Secret params​","keywords":"","version":"Next"},{"title":"🆕 Introduction","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/introduction","content":"🆕 Introduction This section provides detailed information on how you can get started developing on AGIL Ops Hub. It includes how to: Create a new serviceContainerize a serviceGet your service ready for deploymentSet up new infrastructure for your project This applies to all services in general, however, it is common that when you develop a new module/service, it will require some associated front-end components, such as pages or views associated with your new module. Developing on the web has its entire own section, which you can refer to here: Web Development section. Service Endpoints​ Services often need to call other services, and this is almost always done with endpoints (URL's). Since these URL's can change, we store them in a table in the database (in the future, we might have a more sophisticated service discovery mechanism, but for now, we register services to a table in the database and all services should call retrieve service endpoints from this table - aoh_system.endpoint). Accessing the database​ Minimally, your services will still need to take in environment variables to connect to the data endpoint that provides the service endpoints, and also connect to the IAM server to retrieve your access token to grant you access to the system - so these items must still be passed to your service in some configurable way - likely environment variables: Identity Access Management server URLIdentity Access Management client credentials (client ID, secret and realm)Data Endpoint URL (we use Hasura so it is a GraphQL endpoint URL) For details on how to connect to Keycloak to retrieve the access token, go to the IAM section. Alternatively - our services development guide contains source code that retrieves the token (which you will probably be copying).","keywords":"","version":"Next"},{"title":"Setup Service Infra Repository","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/devops_onboard/setup-service-infra","content":"Setup Service Infra Repository Introduction​ note The Service Infra Repository serves as an extension of the Project Infrastructure Repository, and can be referenced by the latter. For instructions to setup the project infrastructure, refer here. This guide provides instructions on how to create a new service infrastructure repository in an existing project. The Service Infra Repository contains IaC which determine how the service (container image) shall be deployed. As part of the CI, the service's IaC is updated when a service's container image is successfully built as a result of changes to the service's source code. Create Service Infra Repository The following instructions assume that you are creating the service infra repo for a service called MYSERVICE. Setting up the service infra repository​ The new repository can be created by one of the following means: Create a new service infra repository directly from template using Github web console​ Create repository directly from template The aoh-service-infra-template repository exist as a GitHub template. If you are hosting your source code on GitHub, you can go to the respective repository links and click Use this template to create a new repository directly from the template. After that, clone the repository to local. Customize template for new repo. ./init-template.sh &lt;PROJECT_NAME&gt; &lt;SERVICE_NAME&gt; then commit and push the changes. Create a new service infra repository from template by cloning to local​ Pre-create empty service infra repository (using name format &lt;PROJECT&gt;-&lt;SERVICE&gt;-infra) on Github and then use service infra template repository to populate it. Clone aoh-service-infra-template repository to local. note For AOH core development, use this: git clone https://github.com/mssfoobar/aoh-service-infra-template # Use this for non-AOH-core development git clone https://github.com/DoisKoh/aoh-service-infra-template Customize template for new repo. ./init-template.sh &lt;PROJECT_NAME&gt; &lt;SERVICE_NAME&gt; Then rename and publish repository ./init-repo.sh &lt;PROJECT_NAME&gt; &lt;SERVICE_NAME&gt; Configure Service Infra Repository With reference to docker-compose.yml file in service's source code repository, configure dev/helm/values.yaml file in the service-infra repository that was setup in the previous step. Search for UPDATE THIS SECTION for areas to be updated.","keywords":"","version":"Next"},{"title":"📘 Logging","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/logging","content":"📘 Logging We standardize our log format to allow searching through logs to be methodical. This is important for us to be able to filter logs in a standardized way. Format​ The logs must be formatted as a JSON - this allows us to perform searches based on the key, as well as pretty print the logs using tools: It must minimally have one of the following log levels: TRACEDEBUGINFOWARNERRORFATAL Example: { &quot;level&quot;: &quot;INFO&quot;, ... } Any other data can then be included as more keys in the log object: Example: { &quot;level&quot;: &quot;ERROR&quot;, &quot;timestamp&quot;: &quot;2023-10-25T03:06:23.423+0000&quot;, &quot;message&quot;: &quot;Failed to secure the crown jewels. Aborting.&quot; } Guidelines​ Info vs Debug levels​ INFO logs should be used sparingly on important events that occur infrequentlyDEBUG logs can be used liberally. Useful logs that are used during application development can be left in production code but at DEBUG or TRACE as these will not be printed in production environments. Retries​ info TLDR; errors that result in retries should be logged as WARN Often, there might be processes that result in errors that are actually anticpated or expected (for example, token expiries, or network errors might be retried) - these errors might be retried, and succeeded. In those scenarios, the errors should be logged with WARN, only the final error should be logged as ERROR, or maybe even FATAL in fatal/panic scenarios.","keywords":"","version":"Next"},{"title":"Conventional Commits","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/source_management/conventional-commits","content":"Conventional Commits Conventional commits is a specification for adding human and machine readable meaning to commit messages. The specification is well documented, read more about conventional commits here. Summary​ The Conventional Commits specification is a lightweight convention on top of commit messages. It provides an easy set of rules for creating an explicit commit history; which makes it easier to write automated tools on top of. This convention dovetails with SemVer, by describing the features, fixes, and breaking changes made in commit messages. The commit message should be structured as follows: &lt;type&gt;[optional scope]: &lt;description&gt; [optional body] [optional footer(s)] Why conventional commits?​ Following conventional commits allows many tools to parse and make sense of commits since they follow an agreed upon convention. It also standardizes them in a format that lets the reader make sense of the commit. On our web server, we use husky to apply a pre-commit check to enforce that developers commit with messages that follows conventional commits. We can then use tools to generate release notes based on these commits - but the quality of these notes will directly be related to the quality of the commit messages and pull requests, so please take this practice seriously!","keywords":"","version":"Next"},{"title":"Update Project Infra Repository","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/devops_onboard/update-project-infra","content":"Update Project Infra Repository Update the project infrastructure to reference the service infra repository that was created in earlier step. Pre-requisites​ Set the following environment variables # name of new service # e.g. ucs / rnr / wfe export MYSERVICE= # URL to service infra repository # e.g. https://github.com/mssfoobar/ar2-ucs-infra export MYSERVICE_REPO_URL= # path to required manifest from within service infra repository # e.g. for helm charts: dev/helm export MYSERVICE_REPO_PATH= # full path to project's IaC repository root # e.g. full path to where the project IaC is checked out export PRJ_IAC_ROOT= # project namespace # e.g. soh / hoc / aoc export PRJ_NAMESPACE= Setup IaC (Do only ONE of the following)​ For service infra using Helm Charts (non-reference) (TBD) Create folder ${PRJ_IAC_ROOT}/helm/${MYSERVICE}// TODO For service infra using Helm Charts (referencing external service-infra repository). Create manifest for myservice: cat &gt; ${PRJ_IAC_ROOT}/apps-children/${PRJ_NAMESPACE}-${MYSERVICE}.yaml &lt;&lt; EOF apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: ${PRJ_NAMESPACE}-${MYSERVICE}-dev namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: ${PRJ_NAMESPACE} name: in-cluster project: appbundle-project-aoh-dev source: path: ${MYSERVICE_REPO_PATH} repoURL: ${MYSERVICE_REPO_URL} targetRevision: main syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true allowEmpty: true selfHeal: true EOF For service infra using manifest files (TBD). Create manifest for myservice: cat &gt; ${PRJ_IAC_ROOT}/apps-children/${PRJ_NAMESPACE}-${MYSERVICE}.yaml &lt;&lt; EOF apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: ${PRJ_NAMESPACE}-${MYSERVICE}-dev namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: ${PRJ_NAMESPACE} name: in-cluster project: appbundle-project-aoh-dev source: path: ${MYSERVICE_REPO_PATH} repoURL: ${MYSERVICE_REPO_URL} targetRevision: main syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true allowEmpty: true selfHeal: true EOF Add repository to ArgoCD​ Engage administrator for the following procedure. From ArgoCD, Settings &gt; Repositories &gt; Connect Repo. Fill up URL of service infra git repository, and credentials.","keywords":"","version":"Next"},{"title":"Publishing Web Components","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/source_management/publishing","content":"Publishing Web Components Manual Publishing​ 1. Test and use the packages before publishing:​ In order to import components from @mssfoobar/aoh-web for testing, use npm link(see https://docs.npmjs.com/cli/v8/commands/npm-link)This allows us to use components from @mssfoobar/aoh-web without adding it as a dependency in package.json. We can then test against our local copy before actually publishing them to the npm registry. cd package npm link cd ../ npm link @mssfoobar/aoh-web 2. Publishing the ready packages:​ The Svelte components here are published to @mssfoobar/aoh-web, to package and publish the components, perform the following steps: First, run svelte-kit package (experimental as of 10th May 2022) to create a package of the files in lib/src. This may be configured via the svelte.config.js file. See: https://kit.svelte.dev/docs/configuration#package &amp; https://kit.svelte.dev/docs/packaging npm run package Then, publish the packaged file. This repository defaults to publishing to our private registry, configured in the .npmrc file for installing npm modules, and the package.json file (publishConfig) for publishing @mssfoobar/aoh-web. cd package npm publish note You will be required to log in to access the private repository, obtain an account from the repository maintainer. Automatic Publishing​ GitHub Actions has been configured to automatically publish the package when a new release is created. For more information on how to publish releases on GitHub, visithttps://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository.","keywords":"","version":"Next"},{"title":"Version Control System","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/source_management/version-control-system","content":"Version Control System This section attempts to explain how we manage our source code, why we do it in this way, and why you have to do it this way too. We use Git as our VCS (version control system) because it is highly mature, widely supported, has well established best-practices, and has many tools that support it. How this ties to Developer Operations​ Since we want our application to be cloud native, we have to have a good degree of infrastructure automation. We'll need to be able to easily scale up or down services by declaratively describing their deployment configurations, and we'll want to have preview servers automatically deployed when we check in our code to faciliate faster development cycles. To achieve these things, our applications are containerized and orchestrated by Kubernetes, and the container images are built automatically by our CI. These images are created whenever a commit occurs on a Git branch that tied to an environment (e.g. a merge request is successfully merged from a feature branch to the 'development' branch, the code from the development branch in that commit would then be built by the CI into an image, and this image will be uploaded to a container registry, and ultimately deployed by Kubernetes). In order for these systems to be run effectively, we adopt the principals ofGitOps, this is necessary for us to develop and deploy our system in a reliable, structured way. What Git Ops?​ GitOps is a practice that makes Git the single source of truth for our infrastructure definitions. This means that we will be able to make changes to the system's deployment 'simply' by looking at the state of the Git repository. By tying the configuration of the system infrastructure and deployment to source code, we also gain the ability to version the infrastructure since they can be tied to individual Git commits. Read more about GitOps here. Branching Strategy​ We are essentially using a form of Git Flow as our branching strategy. This is required because our deployment environments are tied to branches in your Git repository. These branches that get deployed can be seen as 'deployment' branches. For each environment that you want to have automatically deployed, you need a branch. When developers need to make changes to that branch, they check out a new branch using that as the base. They then make changes to their 'personal' feature branch, and when they are done, they make a pull request to merge it back to the base branch, which then gets triggered to be deployed. For example, someone found a typo in the web development branch (this branch deploys to a preview server for us to view changes made to the web server) - for a change to be made, a developer must use the development branch as the base and checkout a new branch with a new name (e.g. fix/ambulance-chart-typo). They then make the changes to fix this typographical error, and open a pull request to contribute this change back to the development branch. This pull request must be reviewed by another developer to ensure it adheres to the projects guidelines and as a primary check to ensure it does not sabotage the system (code formatting and linting should be done automatically by pre-commit hooks). Git Flow is a popular and well documented branching strategy, read more about ithere.","keywords":"","version":"Next"},{"title":"Development","type":0,"sectionRef":"#","url":"/aoh-docs/docs/gis/development","content":"Development // TODO: See multi-root workspaceshttps://code.visualstudio.com/docs/editor/multi-root-workspaces Pre-processing Tools Required​ AutoCAD LTQGIS (Quantum Geographic Information System)ogr2ogr from GDAL Building floor plans typically come in drawing formats such as .dwg, which is predominantly used by AutoCAD. To open this file, you can use LibreCAD or preferably AutoCAD or AutoCAD LT. The goal is to convert the drawing into GeoJSON, along with tag it with additional attributes that conforms to our concept of &quot;Building&quot; geo entities. Processing​ The first thing you should do is to filter out which layers in the .dwg are of interest to you. Open up the file, figure out what you need to keep, which would typically be walls and columns, and write these layers down. In our example, we'll assume you want to keep the following layers: WINDOWWALLCOLUMN Then, save the file as a .dxf. ogr2ogr does not play well with .dwg files, so use LibreCAD or AutoCAD to save the file in .dxf. The result will be a .dxf file that can be processed by ogr2ogr. info We've tried mass converting files using tools but the result was not processable by ogr2ogr. You can experiment to get such tools to help deal with batch conversion but we only ever got ogr2ogr readable conversions from LibreCAD and AutoCAD. Next, use ogr2ogr to convert the drawing to GeoJSON with WGS84 as the coordinate reference system. ogr2ogr does not properly convert .dwg. layer names Windows PowerShellLinux ogr2ogr ` -progress ` -dialect sqlite ` -sql &quot;SELECT * FROM entities WHERE layer in ('WALL', 'WINDOW', 'COLUMN')&quot; ` -f 'GeoJSON' .\\output_file_name.geojson .\\input_file_name.dxf ` -t_srs EPSG:4326 ` -s_srs EPSG:3414 This does not georectify your GeoJSON, but transforms it to be in WGS84 format, which is what GeoJSON is supposed to be in. You can then further transform the points to be properly georectified by running it through ogr2ogr again. But first, we need to find 3 reference coordinates that we can pass to ogr2ogr to do the transformation. cat .\\OMEGA_ONE_STY_7.geojson | jq 'del(.features[].properties.PaperSpace, .features[].properties.SubClasses, .features[].Linetype )' &gt; new.geojson aws codeartifact login --tool npm --repository ar2-npm-group --namespace fortawesome --domain agilrad --domain-owner 991204835536 .DWG &gt; ODAFIleConverter(.DWG) &gt; .DXF &gt; LibreCAD(.DXF) &gt; .DXF &gt; ogr2ogr2(.DXF) &gt; .GeoJSON &gt;","keywords":"","version":"Next"},{"title":"🆕 Introduction","type":0,"sectionRef":"#","url":"/aoh-docs/docs/gis/introduction","content":"🆕 Introduction This section describes how we handle mapping in AGIL Ops Hub.","keywords":"","version":"Next"},{"title":"Signing Commits","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/source_management/verified-commits","content":"Signing Commits To sign commits that are valid under GitHub's rules - you must use a GPG Keypair. Signing commits allows us to verify that commits are made by the user that's committing them. Ideally, all branches should be protected to require commits to be signed. This can be done by configuring GitHub's branch protection rules to ensure commits are only allowed when they have been verified with a GPG signature. The following section explains how you can set your computer up with a GPG keypair to sign your git commits. 1. Generate a GPG Keypair​ To generate a new GPG key on the machine: gpg --full-generate-key 1 for (1) RSA and RSA (default) when prompted on what kind of key you want.4096 when prompted on what keysize you want.0 when prompted how long the key should be valid so that it does not expire (unless you wish to commit to a different policy).y to confirm that the key does not expire if you picked 0.&lt;your name&gt; when asked for your name. This does not need to match the commit name.&lt;email address&gt; when asked for your email, this must match the commit email address.&lt;comment&gt; (optional) some comments to tag onto this key.O for (O)kay when you're done.A dialogue box will appear - key in your passphrase for the key - you MUST remember this passphrase and use it every time to sign a commit. You will be asked to key the passphrase in again to confirm. tip See this reference for more information. 2. Set up the GPG public key​ 2.1. List your existing GPG keys​ To list existing GPG keys on the machine: gpg --list-keys --keyid-format=long gpg --list-secret-keys --keyid-format=long GPG key ID is located after sec rsa4096/ tip See this reference for more information. 2.2. Print the GPG public key​ Print the GPG public key in ASCII armor format: gpg --armor --export &lt;key-ID&gt; tip See this reference for more information. 2.3. Add the GPG public key to GitHub​ Copy the entire output from 2.1 into your GitHub account's settings &gt; keys Select New GPG key and paste the output there. Also, it is highly recommended that you set your account to Vigilant mode to mark any unsigned commits as unverified. tip See this reference for more information. 3. Set up the GPG private key​ Edit the .gitconfig for global configuration. On Windows machines, it should be at %USERPROFILE%/.gitconfig. Or edit the .git/config for each repository's configuration. [user] email = &lt;commit email&gt; signingkey = &lt;key-ID&gt; [commit] gpgsign = true Alternatively, you can use the following commands to edit your git config from the terminal. git config user.email &quot;&lt;commit email&gt;&quot; git config user.signingkey &quot;&lt;key-ID&gt;&quot; git config commit.gpgsign true You can add the --global flag to modify the settings globally (for any/all repositories in your machine). Repository settings will supersede global settings. git config --global user.email &quot;&lt;commit email&gt;&quot; git config --global user.signingkey &quot;&lt;key-ID&gt;&quot; git config --global commit.gpgsign true After that, you will be prompted for a passphrase every time you commit. tip See this reference for more information. List your existing GPG keys​ To list existing GPG keys on the machine: gpg --list-keys --keyid-format=long gpg --list-secret-keys --keyid-format=long GPG key ID is located after sec rsa4096/ This will be useful, especially when you need to view the key ID (used in many of the commands). tip See this reference for more information. Migrate GPG Keypairs​ To export the keypairs from your PC: gpg --export &lt;key-ID&gt; &gt; public_keys.pgp gpg --export-secret-keys &lt;key-ID&gt; &gt; private_keys.pgp To import the keypairs to your other PC: gpg --import &lt; public_keys.pgp gpg --import &lt; private_keys.pgp tip See this reference for more information. Delete GPG Keypairs​ To delete the keypairs from your PC: info You must delete the private key before you can delete the public key. gpg --delete-secret-key &lt;key-ID&gt; gpg --delete-key &lt;key-ID&gt; Or delete the .gnupg file. On Windows machines, it should be at %USERPROFILE%/.gnupg. tip See this reference for more information. Password Entry on Mac​ If you're on MacOS, under the default configuration, gpg might not be able to create a password prompt for you. Run the following commands in sequence to install pinetry-mac, which is a small collection of dialog programs that allow GnuPG to read passphrases and PIN numbers in a secure manner. brew install pinentry-mac echo &quot;pinentry-program $(which pinentry-mac)&quot; &gt;&gt; ~/.gnupg/gpg-agent.conf killall gpg-agent ","keywords":"","version":"Next"},{"title":"Administering OIDC Clients","type":0,"sectionRef":"#","url":"/aoh-docs/docs/iam/clients","content":"Administering OIDC Clients When you create a new service that needs access to the system, you need to create an OIDC client for that service. This will allow access to the system as a user or account associated with that client. For services, we give them access to the system via service accounts. This section describes how you can create an OIDC client for a new service, enable a service account for the OIDC client, and fetch an access token for the service account to grant you access to the system. Creating an OIDC client​ The steps for creating a client is as follows: Click Clients in the menu on the leftClick Create clientChoose a client ID - We prefix the client ID with the domain, for example aoh_web. This is not a Keycloak requirement but a naming convention we follow.Give it a sensible name and descriptionScroll down to Capability config and make sure Client authentication is on and Service accounts roles is checked.Click save. Configuring the client​ You will need to configure your new client to add mappers to have claims added to the token that will let you access Hasura in our system. Read more about authentication in AOH here. Adding mappers​ Add the x-hasura-default-role, x-hasura-allowed-roles and x-hausra-client-id mappers. Click Clients in the menu on the left Click on the client you want to configure Click the Client scopes tab Under Assigned client scope, you will be able to see [client-id]-dedicated as a link, click it to access the Adding the x-hasura-default-role mapper: 5.1. Click Add mapper5.2. Click By configuration5.3. Click User Attribute5.4 Key in 'default-role' in User Attribute5.5 Key in 'hasura_access.x-hasura-default-role' in Token Claim Name5.6 Leave Claim JSON Type as String5.7 Leave Add to ID token checked5.8 Leave Add to access token checked5.9 Leave Add to userinfo checked mappers and scope. Adding the x-hasura-allowed-roles mapper: 6.1. Click Add mapper6.2. Click By configuration6.3. Click User Realm Role6.4 Leave Multivalued checked6.5 Key in 'hasura_access.x-hasura-allowed-roles' in Token Claim Name6.6 Leave Claim JSON Type as String6.7 Leave Add to ID token checked6.8 Leave Add to access token checked6.9 Leave Add to userinfo checked Adding the x-hasura-client-id mapper: 7.1. Click Add mapper7.2. Click By configuration7.3. Click User Session Note7.4 Key in 'clientId' in User Session Note7.5 Key in 'hasura_access.x-hasura-client-id' in Token Claim Name7.6 Leave Claim JSON Type as String7.7 Leave Add to ID token checked7.8 Leave Add to access token checked Manage Service Account Group Mappings​ We use the system role for backend services. For the mappers to work, the service account must have the right attributes on it. We create groups and add the attributes we want to it - this will get merged into the user attributes when a user joins the group, and the mappers we made in the earlier section will map that to the appropraite claims. To get the service account to join the group: Click Clients in the menu on the leftClick on the client you want to configureClick the Service account roles tabClick on the link that says 'To manage detail and group mappings, click on the username&lt;service-account-client-id&gt;', this will take you to the user details page of the service accountClick on the Groups tabClick on Join Group and check system (or any other groups you wish to join) Get an access token to make system calls​ You can get your access token by calling the OIDC endpoint called token, this can be discovered via the OIDC discovery endpoint GET /.well-known/openid-configuration (read more aboutOIDC to learn more). One of the endpoints you will get in return is the token endpoint - which should look like this: http://&lt;your-keycloak-origin&gt;/realms/&lt;your-realm&gt;/protocol/openid-connect/token To make the request, you will need your client credentials, which you can get from the credentials tab in the client details page: To get to this page: Click ClientsClick on the desired client id in the Clients listClick on the credentials tab Make a POST request to that endpoint, passing in the body: client_id: The ID of your client (e.g. aoh_web)grant_type: client_credentialsclient_secret: The secret of your client (retrieved from the client credentials page) The returned result is the access token which you can use as a bearer token to make calls in the system (Hasura).","keywords":"","version":"Next"},{"title":"🖥️ Services","type":0,"sectionRef":"#","url":"/aoh-docs/docs/development/services","content":"🖥️ Services This section provides step-by-step instructions on how you can create a new service in the AGIL Ops Hub framework. Pre-requisites​ tip Pay careful attention to the wsl and Git requirements - all the commands are written for Linux. To follow the steps painlessly on a Windows system, you should install wsl and run the commands in the wsl terminal. Also, make sure Git's credential manager is set up to use Windows Credential Manager. The steps to do this is straightforward and available in the links provided in the Required Tools section. Future plans are to have all scripts written in Python to ensure cross-platform compatibility. Recommended Tools:​ Visual Studio Code A popular text editor with many integrations with the technologies we use. Alternatively, you may use your preferred text editor. Required Tools:​ Windows Subsystem for Linux (Ubuntu Distribution) If you are running on a Windows system, you'll want to install the Ubuntu distribution of WSL (Windows Subsystem for Linux). This is because we'll be giving you Linux commands and scripts to simplify/standardize the steps required to get the project up and running. If you are comfortable with running the equivalent commands on Windows, you may skip this requirement, however, we have scripts/commands ready (e.g. curl-ing the Keycloak Admin API endpoints) for you to run in Linux that might be troublesome to perform on Windows. Go Golang runtime required for development. Git Our version control system for source code. This is installed in Ubuntu by default. If you're using Windows, please install it. You can use GitHub Desktop or your favourite GUI package for this as well. https://learn.microsoft.com/en-us/windows/wsl/tutorials/wsl-git#git-credential-manager-setup If you're using WSL, follow the above tutorial to ensure git in WSL works seamlessly with Windows. This means any password you use when working with git on Windows will also work in WSL. Docker Desktop Contains the container run-time for container images as well as a user-interface for managing containers, and many more additional features such as docker-compose, etc. Even if you are extremely familiar with containerization,Docker Engine alone will probably not be enough as we usedocker-compose. 1. Setting up the repository​ Create a new repository for your service from the template​ The following repositories exist as GitHub templates, if you are hosting your source code on GitHub, you can go to the respective repository links and click Use this template to create a new repository directly from the template, otherwise, you may use the steps below to clone and create a new git repository. 1.1 Clone the repository​ Refer to the pre-requisite section on how to set up git in WSL if your credentials in Windows are not being recognized in WSL. caution Members contributing directly to AGIL Ops Hub will should have access to the repositories in mssfoobar. Members of other projects will require access to the same repository in a different organisation. If you do not have access to the following repositories, approach the maintainers of AOH to request for access. AGIL Ops HubOther Projects git clone https://github.com/mssfoobar/aoh-service-template 1.2 Rename the repository​ This example assumes you are creating a service called solveallyourproblems. You should also replace aoh with your project or organisation name. mv aoh-service-template aoh-solveallyourproblems 1.3 Change the .git repository to your own new one​ cd aoh-solveallyourproblems rm -rf .git git init git checkout -b main git add . git commit -m &quot;initial commit&quot; Replace [your-remote-url] with the actual URL of your remote Git repository (e.g. git remote set-url origin https://github.com/mssfoobar/aoh-solveallyourproblems) git remote set-url origin [your-remote-url] 2. Create a Keycloak client for your service​ Your new service will need to authenticate itself with Keycloak in order to access other services in our system (e.g. to connect to Hasura to write or read data in the database). To do this, it needs to register itself with Keycloak as a new client and exchange its credentials for an access token that must be sent with each request to other services. 2.1 Prepare information and executables​ 2.1.1 Install jq if you don't have it (it is a json query)​ sudo apt install jq 2.1.2 Get all the information you need for your service​ Prepare the following information: Service ID (e.g. aoh_solveallyourproblems)Service Label (e.g. AGIL Ops Hub Solve All Your Problems Service)Service Description (e.g. This service will solve all your problems.)Keycloak Username: This is the username to log in to your admin user in Keycloak (e.g. user)Keycloak Password: This is the password to log in to your admin user in Keycloak (e.g. password123)Keycloak URL: This is the address of your Keycloak server (e.g. http://iam.dev.aoh)Keycloak Realm: This is the Keycloak realm for your project (e.g. aoh) 2.1.3 Run the quickstart script​ Ensure you are in the ./config folder, it contains the following template configuration files that are used in the ensuing commands: client.jsongroup.jsonrole.jsonquickstart.sh The last file is a quickstart script that you can run to perform all the commands in section 2. ./quickstart.sh The successful command will return you the Client UUID, Client Secret and Client Service Account Name. You should copy this secret and place it in your .env file in step 5.1 to save you the trouble of executing the command to retrieve the secret again. This will now allow your service account to have the correct scopes to access the system with its new role. 3. Configure Internal Table Permissions​ The permission for the role must be configured in Hasura - configure CRUD access to desired tables. Refer to the following link on how to configure permissions in Hasura:https://hasura.io/docs/latest/auth/authorization/permissions/ 4. Understanding the Project Structure​ Below is a summary of our project structure, which follows Golang's standards. For more information, see:https://github.com/golang-standards/project-layout cmd​ This is where the application code sits. This typically calls reusable packages from the pkg folder. If you have types, classes, etc. that can be reused in other applications, it should reside in pkg instead. It's common for cmd to have a small main function that simply imports code from pkg. pkg​ This should contain library code that can be used by other applications. Ideally, your package code could function as an SDK which can be imported and used in other Golang projects to access and process data in AOH. docker​ Your service may contain multiple packages in your pkg folder, each of these packages should have its own Dockerfile. In this sample, we'll just have one called service.Dockerfile, you should rename this accordingly. (e.g. solveallyourproblems.Dockerfile) docker-compose​ The docker-compose file contains all the information needed for Docker to build a container image for your service. You will need to 5. Start writing your service​ Congratulations, you can now start writing your service. Compiling and running your service requires the Go tool, which you should already have installed. See required tools above. Without the Go tool, you won't be able to build or run your Go program. You may use any IDE you're comfortable with (such as GoLand), however, we recommend Visual Studio Code to keep all of us on the same development environment, which makes troubleshooting simpler. 5.1 Create a new .env file​ This environment file is not meant to be checked in, and is used entirely for your local development. A sample .envfile is provided to you called .env.sample, with all the default environment variables we used to set up the database connection. Copy this file and rename it to .env: cp .env.sample .env Then, open the file and fill in the values accordingly. One of the values you will require is the client secret. You can either use the Keycloak admin UI to retrieve this, or run the following script: note This assumes you already have TOKEN, KEYCLOAK_USERNAME, KEYCLOAK_PASSWORD, KEYCLOAK_URL, KEYCLOAK_REALM,CLIENT_ID, and CLIENT_UUID variables set. Otherwise, refer to section 2.3and section 2.4 for details on how they should be set. Use the token and the id to get the secret: curl -s -X GET -H &quot;Content-Type: application/json&quot; \\ -H &quot;Authorization: Bearer $TOKEN &quot; \\ $KEYCLOAK_URL/admin/realms/$KEYCLOAK_REALM/clients/$CLIENT_UUID/client-secret | echo IAM_CLIENT_SECRET is: $(jq -r &quot;.value&quot;) 5.2 Update the template service's names to your new service's name​ To get started, search for the text TODO in the template project - in VSCode, the hotkey is Ctrl + Shift + F to search for text in all folders. There, you'll find comments on what you should replace (mostly code relating to the name of your service, and the name of the Go module) ./go.mod Update your module name../cmd/service-name/main.go Rename the 'service-name' folder appropriately (to whatever your package should be called)./pkg/constants Add or remove any constants required../docker/service.Dockerfile Rename all the occurrances of servicename to an appropriate name for your service. Alsom add any additional build steps if required../docker-compose.yml Add any additional environment variables your service might need. 6. Provide a readiness and liveness endpoint​ Every service is required to provide a readiness and liveness endpoint. The purpose of these is so Kubernetes can periodically hit these endpoints to check if your service is alive, or ready. Reference: https://kubernetes.io/docs/tasks/configure-pod-container/configure-liveness-readiness-startup-probes/ The template code provides these endpoints to you via the /v1/info/liveness and /v1/info/readiness endpoints. You will likely not need to change the liveness endpoint, however, if your services requires additional information or steps to perform before it is ready to serve requests, you will need to make sure that all these steps are done before you return a 200 OK on the readiness endpoint. 7. Handling CORS​ AGIL Ops Hub handles CORS with Traefik - you are not expected to handle CORS in your backend services. However, if you have specific needs that requires CORS to be handled in your service itself, we recommend you use a middleware for ginsuch as https://github.com/gin-contrib/cors. During local development, you may run into issues with CORS in your local environment (e.g. when your browser calls your locally run service with a different origin). In preview and production environments, Traefik will handle this for you, however, to circumvent this during local development, you can disable CORS for your browser. For Chrome, you can do so by executing a new instance of Chrome with the following two flags: --disable-web-security --user-data-dir=&quot;Path:\\to\\any\\existing_folder&quot; To run with disable-web-security, you must also run with Chrome user-data-dir, specifying a directory for Chrome to use to store your special 'disabled security' profile. Just create a folder anywhere that's convenient for you and use that folder to store the proflile. Example full command: chrome.exe --disable-web-security --user-data-dir=~\\tmp\\ tip You do not need to close existing instances of Chrome before running this command. If it isn't working for you, double-check that the --user-data-dir path you are providing exists. As of version 118.0.5993.118 of Chrome, you should expect to see a warning underneath the address bar stating:You are using an unsupported command-line flag: --disable-web-security. Stability and security will suffer. 8. Preparing your service for deployment​ It is the responsibility of developers to ensure that their docker-compose.yml file is fully up to date and that their services can be built and run as a container. The .github/workflows folder contains GitHub actions to automatically build your service into a container image and deploy this container image to ghcr.io (GitHub container registry), the scripts therein should be managed by your DevOps engineer (which may or may not be you as well). For details on initial deployment of this service, see the documentation on service deployment configuration. For details on source code management, visit thedocumentation on Source Management 9. Documenting your service​ Your service should be documented as in Open API yaml file. Our workflow consists of us creating Postman Collections to share and test our API's, then converting them to Open API files. TODO: - Expand on OpenAPI documentation, commenting in code with gin-swagger could allow us to generate Open API files from code, which can then be imported into Postman, and also have client SDK's generated for TypeScript - https://github.com/swaggo/gin-swagger ","keywords":"","version":"Next"},{"title":"🆕 Introduction","type":0,"sectionRef":"#","url":"/aoh-docs/docs/iam/introduction","content":"🆕 Introduction Our system uses Keycloak as the identity access management server. Keycloak is an open-source identity and access management (IAM) solution developed by Red Hat. Their enterprise fork is called Red Hat Single-Sign On (SSO). It follows the OpenID Connect and OAuth 2.0 standards and it highly extensible, allowing it to integrate seamlessly with many other applications, frameworks, and even perform user federation and integration with LDAP providers. OpenID Connect​ OpenID Connect is an identity layer built on top of the OAuth 2.0 framework. It is an open standard that provides a secure and standardized way for user authentication and authorization in modern web applications and APIs. By utilizing OpenID Connect, we'll be able to integrate with other systems that follow the OIDC standard, and also even add support for social logins. We can even swap out Keycloak for a different identity provider (e.g. Google, Facebook's, Auth0, etc.) should the need arise. Keycloak​ As Keycloak acts as our IAM server, it handles the generation of JSON Web Tokens JWT's as well as authentication. Managing Keycloak itself is a large topic, and requires some understanding of OIDC authentication flows, as well as the the security concepts surrounding authentication using JWT's. info You can read more Keycloak, OIDC, and OAuth here: Keycloak: https://www.keycloak.org/documentationOIDC and OAuth: https://developer.okta.com/blog/2019/10/21/illustrated-guide-to-oauth-and-oidc How do we use Keycloak?​ We use Keycloak's User Federation system and their User Storage Service Provider Interface to integrate with our database to retrieve users. This is done by implementing the Java interfaces for the User Storage SPI, and placing the built .jar file in the Keycloak server. info Find out more about Keycloak server development: User Storage SPI: https://www.keycloak.org/docs/latest/server_development/#_user-storage-spiUser Federation: https://www.keycloak.org/docs/latest/server_admin/#adding-a-provider You can find the source code for our User Storage Provider Interface implementation here: https://github.com/mssfoobar/ar-keycloak-providers Authentication in AGIL Ops Hub​ In the current iteration of the system, users are part of teams and have roles. In terms of database relationships, a user can be part of 1 or more teams, and they can have 1 or more roles in that time. However, when accessing the system, they must always assume 1 role to do so. Keycloak does not have the concept of 'teams', but they do have 'groups'. We map each users' team to Keycloak groups, and each users' role to Keycloak's roles. Note however, that in Keycloak, 'groups' are aggregates of roles - which is not that case in our system for 'teams'. Each user is associated to a team and role via the 'membership' table. The user entity has a 'membership_id' foreign key; this refers to the active membership that the user has. Meaning, the exact team and role he/she is assuming whilst accessing the system. This active membership role is mapped to the x-hasura-default-role claim in the access token and that role claim is used to make requests in the system (GraphQL requests to Hasura). Hasura performs the authorization via the permissions tables for each entity/table. A user can have multiple memberships (this relationship is stored as the user_id foreign key in the membership table). The User Storage SPI maps these memberships to the x-hasura-allowed-roles claim. This means that you may passx-hasura-role in the request header to Hasura to assume a different role for that request. info Visit https://hasura.io/docs/latest/auth/authorization/index/ for more information on how Hasura handles roles.","keywords":"","version":"Next"},{"title":"Roles & Groups","type":0,"sectionRef":"#","url":"/aoh-docs/docs/iam/roles_and_groups","content":"Roles &amp; Groups Keycloak has the concept of Roles and Groups; a user can have roles or groups, and a group can contain multiple roles. Roles in AOH (AGIL Ops Hub)​ As explained in the introduction, AGIL Ops Hub uses roles, and allows users have different roles depending on what team they are in. These roles are match the roles in Hasura and is used to evaluate permission policies that control what tables the user is allowed to access. Combined with Hasura's custom check for permissions, we have the ability to very flexibly control what tables users are allowed to access. The roles in AGIL Ops Hub map directly to roles in Keycloak - they are the same. However, the way we use groups in keycloak is the same as roles. This is due to a quirk of how Keycloak assigns attributes to users - it does not merge attributes from roles into the user attributes, but it does so from the groups. This means that if we want to create custom claims based on the user's role by reading their role attributes, it won't work. This is why we put users in groups as if they are their role. However, we still have to give users a role, as users are only allowed to assume roles based on theirx-hasura-allowed-role claim. Creating a new role​ To create a new role, you must create both a role, and a matching group, and assign the `default-role`` attribute in that group. Creating a role in Keycloak​ Click Realm roles on the left menuClick Create roleGive the role a name and helpful descriptionClick save Creating a group in Keycloak​ Click Groups on the leftClick Create groupGive your group a name, this should match the role (to avoid confusion)Click on your newly created groupClick on the attributes tabCreate a new key, value pair with the key as default-role and the value as the name of your role","keywords":"","version":"Next"},{"title":"🫶 Collaboration","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/collaboration","content":"🫶 Collaboration Outline​ This article outlines the modes of collaboration for AOH development. AOH shall avail its packages (eg. NPM registries and container images) to developers. The project shall setup and maintain the following repositories: Infrastructure code repository Project Infra Repo. This contains the over-arching project deliverable setup, and serves as the gatekeeper to the project's deliverables.Each module/service within the project having: source code repository Dev Src Repoinfrastructure code repository Dev Infra Repocontainer image registry Dev Container Registry The project shall exercise full control over their product's development and release processes. The appropriate permissions can be governed via access control to the repositories. Should the project engage external parties, these parties can have autonomy of their repositories/registries. NOTE: external parties' access should be restricted to artifacts marked under External only. Development​ For services to be incorporated into AOH. The following are generally required: Source codeInfrastructure codeHasura metadata + DB Initialization Scripts includes service's schema and graphql references","keywords":"","version":"Next"},{"title":"🆕 Introduction","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/introduction","content":"🆕 Introduction Introduction​ AGIL Ops Hub or AOH is a platform for developing, testing, and deploying C2 (Command &amp; Control) systems. Out of the box, we aim to support many different use-cases but mainly with C2 systems in-mind. Features (Planned)​ Charting [✔️]Configurable dashboards [✔️]Text, Audio, and Video chat [✔️]Real-time Mapping [✔️]Recording &amp; Playback [✔️]Incident Management [✔️]Complex Event ProcessingSimulation Designer Suite (Planned)​ Form Designer [✔️]Workflow Designer [✔️]Rule DesignerOptimization DesignerSimulation Designer Philosophy​ For the system to be successful, we require buy-in from all stakeholders, including and especially developers. The stack we have is modern and aimed to please as many parties as we can. Clients' demands based on current technology trends; which emphasizes secure, scalable, flexible systems, are met with the micro-service architecture. To help with developers' needs, we focus on providing strong documentation, powerful modern tooling, and automation wherever it makes sense.","keywords":"","version":"Next"},{"title":"📐 Architecture","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/architecture","content":"📐 Architecture This section describes how the web service interacts with other services, ignoring infrastructural modules such as Kubernetes, pods, containers etc. As the web server for your framework, it is the primary interface that users will interact with, and essentially depends on everything else in the system to function properly. However, the components in the above diagram are the main, critical pieces that the web server interfaces with. The following is a brief description of the pieces in the diagram Browser - he clients that connect to the web serverWeb - the AGILRAD 2.0 web serverHasura - the server providing Graph QL API's - essentially the serving the purpose of what application servers doIdentity Access Management - we use Keycloak for this, it is the identity provider for your usersApplication Database - the database hosting application data The initial unauthenticated request from the browser to the web server will result in the user being redirected to the IAM to log in. The authentication process is faciliated using the OIDC Authentication Code Flow. After this flow is completed, the web server will receive the JWT tokens for the user, and set them as cookies on the user's browser. If the user's request to the desired route is valid and authorized, the initial page render will be done server-side. This initial render gets data from Hasura using the user's JWT to provide authentication. Subsequent navigation is handled on the browser and all Graph QL requests are sent directly to a Hasura instance that accepts cookie-based authentication instead (this is for security reasons). Any updates in data is also done via Graph QL subscriptions over WebSocket. This allows for the interactive SPA (Single-Page Application) experience whilst also providing very fast initial response from server-side rendering.","keywords":"","version":"Next"},{"title":"🔥 Known Issues","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/reference/known-issues","content":"🔥 Known Issues This list will be populated when we reach a stable version.","keywords":"","version":"Next"},{"title":"✔️ Runbook","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/reference/runbook","content":"✔️ Runbook AR2 Web Server​ Running the server in dev mode​ WindowsmacOSLinux npm run clean; npm run generate; npm run dev Database Administration​ Connecting to Postgres Database​ psql -h &lt;HOST&gt; -d postgres -U postgres -p &lt;PORT&gt; e.g. with default Postgres container psql -h localhost -d postgres -U postgres -p 5432 Connecting to Postgres Database &amp; Running Command Immediately​ psql -h &lt;HOST&gt; -d postgres -U postgres -p &lt;PORT&gt; -c &lt;COMMAND&gt; e.g. with default Postgres container psql -h localhost -d postgres -U postgres -p 5432 -c &quot;CREATE DATABASE ar2;&quot; Start psql connection with an SQL file​ This is useful for loading dumps in the form of .sql files. psql -h &lt;HOST&gt; -d ar2 -U postgres -p &lt;PORT&gt; -f &lt;PATH_TO_SQL&gt; Retrieving Database Dump from Hasura​ Reference: https://hasura.io/docs/latest/api-reference/pgdump/ Do a POST request to http://{{hasura_url}}/v1alpha1/pg_dump with the following body: note Remember to supply any required credentials through the header (e.g. x-hasura-admin-secret) POST /v1alpha1/pg_dump HTTP/1.1 Content-Type: application/json X-Hasura-Role: admin { &quot;opts&quot;: [ &quot;--no-owner&quot;, &quot;--no-acl&quot;], &quot;clean_output&quot;: true, &quot;source&quot;: &quot;ar2&quot; } Docker &amp; Docker Compose​ Build + Create + Start (detached)​ docker compose up -d --build Stop + Remove (images) + Remove (volumes)​ docker compose down --rmi local -v ","keywords":"","version":"Next"},{"title":"Docker","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/docker","content":"Docker We use Docker to run our containers. What is Docker?​ Docker is a set of platform as a service products that use OS-level virtualization to deliver software in packages called containers. Why we've chosen Docker​ We need containerization to perform continuous integration and deployment on Kubernetes, and Docker completely dominates the market for containers. It is the most popular containerization platform, and has many resources available for it. The main alternative would be to go with Open Shift but it is something we have yet to explore. More Reading​ Official Docker Website","keywords":"","version":"Next"},{"title":"💀 Pitfalls","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/reference/pitfalls","content":"💀 Pitfalls Common mistakes, errors and pitfalls. Svelte &amp; Svelte Kit​ 'document' or 'window' is not defined​ 'document' or 'window' is not defined SvelteKit has a hybrid server-side-rendering and client-side-rendering model - the initial page is rendered on the server, then subsequent execution of .svelte components is done on the client-side. This method provides a great experience for site visitors but introduces some additional complexity. One important thing to note is that certain libraries depend on the DOM to load (e.g. gridstack.js), loading on the server intially results in errors due towindow or document not existing on the server (nodejs). To circumvent this, we need to use dynamic imports. Asynchronous onMount functions​ If you run an onMount function asynchronously, it returns a promise instead of a function. This will result in the returned 'function' not being called. See reference discussion: https://github.com/sveltejs/svelte/issues/4927 onMount(async () =&gt; { bar = await baz(); return () =&gt; { console.log(&quot;I'm never called!&quot;); }; }); To get around this, you can create and run an async function immediately inside onMount: onMount(() =&gt; { async function foo() { bar = await baz(); } foo(); return () =&gt; console.log(&quot;Now, I do get called when destroyed.&quot;); }); Svelte Store Usage​ Any store which is meant to be specific to each individual client only always be set in onMount or in a is (browser)check. This is because stores run on the server are global. See reference discussion: https://github.com/sveltejs/kit/discussions/4339 caution Understanding this is very important for avoiding bugs and potential leaking of sensitive information on the browser. Please look through the linked discussion thoroughly. Keying {#each} Blocks​ If you use Svelte with any other framework that might manipulate the DOM or have an internal representation of the DOM (e.g. GridStack), you will very likely run into issues with this. See how and why to use keyed each blocks: https://svelte.dev/tutorial/keyed-each-blocks By default, when you modify the value of an each block, it will add and remove items at the end of the block, and update any values that have changed. That might not be what you want. This will cause the frameworks' representations of the DOM to go out of sync. Adding a unique identifier (or 'key') to the each block will allow Svelte to keep track of which element needs to be removed. Testing before merging code with adapter-node​ Running the following commands to test your production build is not enough: npm run build npm run preview Since our application is meant to be run on nodejs and is built with adapter-node, the appropriate way to run the app is with: node build This runs and index.js file inside the git-ignored 'build' folder. There is still the potential for your app to fail at this point, so please test with node build. Errors when executing npm run dev​ ERR_MODULE_NOT_FOUND Sample Error: error when starting dev server: Error [ERR_MODULE_NOT_FOUND]: Cannot find package '...' imported from ... This type of error (ERR_MODULE_NOT_FOUND) typically happens when a new npm package is included into the project and you were not informed of it, or might have forgotten to add/install it. You shouldbe able to resolve it simply by installing the new package. Run: npm install No matching export in &quot;src/generated/types.ts for import &quot;...&quot; Sample Error: X [ERROR] No matching export in &quot;src/generated/types.ts&quot; for import &quot;MapDataDocument&quot; src/routes/(app)/map-view/+page.ts:4:28: 4 │ import { type MapDataQuery, MapDataDocument } from '$generated-types'; You need to run npm run generate to generate the types from the GraphQL schema. This is likely to occur when you've pulled other developers' changes and they might have added new queries to the project. Error when executing npm run getschema​ endpoint is required: gq &lt;endpoint&gt; Sample Error: &gt; @mssfoobar/aoh-web@0.2.0 getschema &gt; env-cmd -f .env.development -x gq -H &quot;X-Hasura-Admin-Secret: $PUBLIC_HASURA_ADMIN_KEY&quot; --introspect &gt; schema.graphql » Error: endpoint is required: `gq &lt;endpoint&gt;` Our usage of the gq command (which is 'graphqurl') requires the environment variable GRAPHQURL_ENDPOINT - it is not set in your .env.development file, check theaoh-web wiki on what value you might need to set for it. Executing query... error: invalid x-hasura-admin-secret/x-hasura-access-key Sample Error: &gt; @mssfoobar/aoh-web@0.2.0 getschema Executing query... error Error: { errors: [ { extensions: [Object], message: 'invalid x-hasura-admin-secret/x-hasura-access-key' } ] } Calling the introspection endpoint to get the full schema requires privileged access. You need to set thePUBLIC_HASURA_ADMIN_KEY environment variable in your .env.development file, check theaoh-web wiki on what value you might need to set for it. Error when executing npm run generate​ Sample Error: ✔ Parse Configuration ⚠ Generate outputs ❯ Generate src/generated/types.ts ✔ Load GraphQL schemas ✔ Load GraphQL documents ✖ Not all operations have an unique name: EmergencyDepartmentData This error occurs when you run npm run generate when you already have generated types, and graphql-codegen seems to attempt to merge the types instead of replace them. Delete your /src/generated folder (or just the /src/generated/types.ts) and run the command again, it should generate the types just fine. You can do this by running npm run clean, which deletes the generated types. This can also occur when not all operations have a unique name - especially if you had multiple identical operations in different files, then changed one of them without renaming them (so you really ended up with 2 different operations with the same name). Missing user role​ Users' current role is based on their active_membership relationship to a membership they have. This is through the foreign key membership_id in the user_user table. It is highly likely that the user has no active membership set (no membership_id set). This relationship is used by Keycloak to populate the access token with the claims for the user's role. GraphQL​ Reusing GraphQL Queries​ urql caches queries and uses the same result for operations that have the same queries/subscriptions/mutations. For example, if you create a subscription like so: Example GraphQL query const ExampleSubscriptionDocument = gql`subscription ExampleSubscription { example_table { id } }` And mutate it in multiple places: ExampleA/index.svelte pipe( client.subscription&lt;ExampleSubscriptionSubscription&gt;(ExampleSubscriptionDocument. {}), subscribe(async result =&gt; { myDataInA = result?.data?.unshift(); } ); ExampleB/index.svelte pipe( client.subscription&lt;ExampleSubscriptionSubscription&gt;(ExampleSubscriptionDocument. {}), subscribe(async result =&gt; { myDataInB = result?.data?.unshift(); } ); The objects in result are actually shared - so mutating the resulting data arrays like in the example above will result in the second operation not receiving the same values. This is because urql shares those operations for performance reasons (so we don't need to have multiple operations with the same GraphQL query). This however, means any mutations in one, will affect the other. Knowing this, you should either copy out values for queries that will be used in multiple areas, or share the value directly via other mechanisms (such as Svelte stores).","keywords":"","version":"Next"},{"title":"🦖 Docusaurus","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/reference/docusaurus-reference","content":"🦖 Docusaurus The following is a list of useful examples for reference when editing the markdown files on Docusaurus. note Formatting is very important for MDX: https://github.com/facebook/docusaurus/issues/3890You must follow the formatting somewhat strictly (especially excluding spaces at the start of the sentence). Images​ Code to render above example: ![Svelte Logo](/img/svelte_logo.svg) Tabs​ WindowsmacOSLinux shutdown -t 0 -s Code to render above example: &lt;!-- You must import the React components --&gt; import Tabs from '@theme/Tabs'; import TabItem from '@theme/TabItem'; &lt;Tabs&gt; &lt;TabItem value=&quot;Windows&quot; label=&quot;Windows&quot; default&gt; ```bash shutdown -t 0 -s ``` &lt;/TabItem&gt; &lt;TabItem value=&quot;macOS&quot; label=&quot;macOS&quot;&gt; ```bash sudo shutdown -h now ``` &lt;/TabItem&gt; &lt;TabItem value=&quot;Linux&quot; label=&quot;Linux&quot;&gt; ```bash sudo shutdown -h now ``` &lt;/TabItem&gt; &lt;/Tabs&gt; Codeblocks​ /src/components/HelloCodeTitle.js function HelloCodeTitle(props) { return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;; } Code to render above example: ```jsx title=&quot;/src/components/HelloCodeTitle.js&quot; function HelloCodeTitle(props) { return &lt;h1&gt;Hello, {props.name}&lt;/h1&gt;; } ``` Equations​ I=∫02πsin⁡(x) dxI = \\int_0^{2\\pi} \\sin(x)\\,dxI=∫02π​sin(x)dx Code to render above example: $$ I = \\int_0^{2\\pi} \\sin(x)\\,dx $$ Admonitions​ note Some content with Markdown syntax. Check this api. tip Some content with Markdown syntax. Check this api. info Some content with Markdown syntax. Check this api. caution Some content with Markdown syntax. Check this api. danger Some content with Markdown syntax. Check this api. Code to render above examples: :::note Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::tip Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::info Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::caution Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: :::danger Some **content** with _Markdown_ `syntax`. Check [this `api`](#). ::: Line Highlighting​ function HighlightSomeText(highlight) { if (highlight) { return &quot;This text is highlighted!&quot;; } return &quot;Nothing highlighted&quot;; } function HighlightMoreText(highlight) { if (highlight) { return &quot;This range is highlighted!&quot;; } return &quot;Nothing highlighted&quot;; } Code to render above example: ```js function HighlightSomeText(highlight) { if (highlight) { // highlight-next-line return &quot;This text is highlighted!&quot;; } return &quot;Nothing highlighted&quot;; } function HighlightMoreText(highlight) { // highlight-start if (highlight) { return &quot;This range is highlighted!&quot;; } // highlight-end return &quot;Nothing highlighted&quot;; } ``` Line Numbering​ import React from &quot;react&quot;; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent; Code to render above example: ```jsx {1,4-6,11} showLineNumbers import React from &quot;react&quot;; function MyComponent(props) { if (props.isBar) { return &lt;div&gt;Bar&lt;/div&gt;; } return &lt;div&gt;Foo&lt;/div&gt;; } export default MyComponent; ``` ","keywords":"","version":"Next"},{"title":"Apache ECharts","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/echarts","content":"Apache ECharts We use Apache ECharts as our primary charting library. What is ECharts?​ ECharts, a powerful, interactive charting and visualization library for the web. Why we've chosen ECharts​ Compared to other popular charting libraries like chart.js, ECharts has a very rich set of charts and visualization options. It is highly configurable, full of a large variety of chart types, has a great default clean look, and is performant; with incremental rendering and data streaming, it even has the ability to render tens of millions of data points. It also has the added benefit of being open source. Using ECharts​ With Svelte &amp; Svelte Kit, you can import charts and components from ECharts into your app: let chartContainer; onMount(async () =&gt; { const echarts = await import('echarts'); let chart = echarts.init(chartContainer); let option = { series: [ { type: 'pie', data: [ { value: 335, name: 'Direct Visit', }, { value: 234, name: 'Union Ad', }, { value: 1548, name: 'Search Engine', }, ], }, ], }; chart.setOption(option); }); Remember to bind the element reference to the chart container that ECharts uses: &lt;div bind:this={chartContainer} /&gt; note Because Svelte Kit code must be isomorphic, and ECharts assumes that the code is run in the browser, you can only can only import ECharts in the browser, hence, the await import('echarts') within the onMount function. More Reading​ Official ECharts Website","keywords":"","version":"Next"},{"title":"Docusaurus","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/docusaurus","content":"Docusaurus We use Docusaurus to generate our documentation from Markdown files. What is Docusaurus?​ Docusaurus is a React-based static site generator from Facebook. It creates documentation sites from Markdown files and provides many convenient rendering options, theming, as well as versioning and search. Why we've chosen Docusaurus​ Docusaurus is open source, easy to pick up, and looks great. It is extremely popular and is used for documentation in many open source projects. More Reading​ Official Docusaurus Website","keywords":"","version":"Next"},{"title":"Faker","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/fakerjs","content":"Faker We use Faker to generate fake (but realistic) data for testing and development. What is Faker?​ Faker is a popular library that generates fake (but reasonable) data to be used for things such as Unit Testing, Performance Testing, Building Demos and working without a completed backend. Using Faker​ Import Faker into your app: import { faker } from '@faker-js/faker'; Use Faker to generate random names, emails and more: const randomName = faker.name.findName(); // Rowan Nikolaus const randomEmail = faker.internet.email(); // Kassandra.Haley@erich.biz More Reading​ Official Faker Website","keywords":"","version":"Next"},{"title":"Font Awesome","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/fontawesome","content":"Font Awesome We've chosen Font Awesome to be our primary icon library. What is Font Awesome?​ Font Awesome is a font and icon library and toolkit based on CSS. Why we've chosen Font Awesome​ Font Awesome has the ability to render icons efficiently as font glyphs - this has the advantage of having them all be indefinitely scalable, due to them being vector based, and also recoloured. This means we can re-theme these icons programmatically. Font Awesome also has a new framework to allow stacking and animations of icons, providing us even more power and flexibility (e.g. we can quickly create loading spinners, or unread message counts overlaid on icons). Using Font Awesome​ The typical usage of Font Awesome uses &lt;i&gt; tags with fa- classes, however, this requires the entire set of icons to be loaded as a Font. If we're only using a few icons from the set, this can be very wasteful in terms of space-efficiency. Instead, we can include the SVG directly into the page with the following method: Import icon from @fortawesome/fontawesome-svg-core and import the icon you'll be using (e.g. faClock): import { icon } from '@fortawesome/fontawesome-svg-core'; import { faClock } from '@fortawesome/pro-solid-svg-icons'; Use Svelte's HTML tags to render the icon directly. {@html icon(faClock).html} danger Svelte doesn't perform any sanitization of the expression inside {@html ...} before it gets inserted into the DOM. To avoid the risk of XSS attacks, do not include any user input in the expression, or ensure you manually escape it. More Reading​ Official Font Awesome Website","keywords":"","version":"Next"},{"title":"GraphQL","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/graphql","content":"GraphQL GraphQL is a query language for that we use for our APIs. What is GraphQL?​ GraphQL is a query language and server-side language runtime for APIs that prioritizes giving clients the data they request. GraphQL is designed to make APIs fast, flexible and developer-friendly. Why we've chosen GraphQL​ With GraphQL, the structure of your query is exactly the same as the result it returns. You don't get partialpayloads, that you have to sift through on the client side. This makes processing the data on the client side much easier and more flexible (depending what GraphQL queries we expose to the client). GraphQL also allows has an introspection feature, which fuels API discovery and makes writing queries much easier. More Reading​ Official GraphQL Website","keywords":"","version":"Next"},{"title":"Others","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/others","content":"Others A list of less critical dependencies used in AgilRad 2.0 Web. FxTS​ FxTS is a functional library for TypeScript/JavaScript programmers. Official FxTS website dayjs​ DayJS is a simple JavaScript library to handle manipulation of dates and times. It is a fast 2kB alternative to Moment.js with the same modern API. Official Day.js Website chromajs​ chroma.js is a small-ish zero-dependency JavaScript library (13.5kB) for all kinds of color conversions and color scales. Official chroma.js Website change-case​ Transform a string between camelCase, PascalCase, Capital Case, snake_case, param-case, CONSTANT_CASE and others. Official change-case Website","keywords":"","version":"Next"},{"title":"MapBox","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/mapbox_openlayers","content":"MapBox MapBox is a library we use to render dynamic maps. What is MapBox?​ MapBox is a client-side JavaScript library that uses WebGL to render interactive maps from vector tiles. You can use Mapbox to display Mapbox maps in a web browser or client, add user interactivity, and customize the map experience in your application. Why we've chosen MapBox​ Apart from MapBox, we also have the option of using OpenLayers and Leaflet.js, which are both open source. Leaflet is very easy to get started with, however, for more complex use cases, OpenLayers or MapBox is a better choice. MapBox actually uses a lot of Leaflet.js code to render tiles, however, MapBox hasMapBox GL JS - adding a rich graphics layer which tips us toward using MapBox over OpenLayers. More Reading​ Official OpenLayers Website","keywords":"","version":"Next"},{"title":"Technology Overview","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/overview","content":"Technology Overview To support the infrastructure to allow our systems to be distributed, scalable, durable, performant, maintainable, and to allow automated testing, security scanning and alerting, and deployment, many different pieces need to fit together to support the backbone of the system. The following is an evolving list of technologies chosen to support development of the AOH platform. Platform Infrastructure info Items in the list that do not have a tick [✓] are prospective candidates that are being evaluated. Container Management​ Kubernetes [✓] In order to support a micro-services architecture, we need to be able to orchestrate the automatic scaling or services. Our services need to be containerized and monitored, stood up or down depending on their needs, and have their traffic routed accordingly. The de-facto standard across the industry for container orchestration is Kubernetes, which is what we've adopted. Docker [✓] Container engine. Ingress Controller​ Traefik Kubernetes Ingress Controller [✓] We use Traefik as a reverse proxy and load balancer to manage access to cluster services in Kubernetes. It is open source, well-documented, and is actively maintained and developed by a large community of users and contributors. NGINX Service Mesh​ Gloo Mesh We plan to use Gloo Mesh for service discovery and observability - this will be required to manage the communication between many services in a structured, reliable way. Secrets Management​ No candidate Access Control​ Access Control Tables [✓]Hasura Permissions [✓] Continuous Integration​ GitHub Source control and collaboration. GitHub Actions [✓] Task runner for automation. Terraform Terraform codifies cloud APIs into declarative configuration files. Continuous Deployment​ ArgoCD [✓] Declarative continuous delivery with a fully-loaded UI. Monitoring &amp; Logging​ Kibana [✓] The interface for querying/filtering and visualizing log data. Elasticsearch [✓] For providing queries across logs. Fluentd [✓] Collects and aggregates logs. Monitoring​ Prometheus [✓] Monitors and triggers alerts based on metrics such as CPU usage, memory usage, response times, error rates, etc. Visualization​ Grafana [✓] Used for visualization of the health of the system (e.g. request rate, CPU usage, etc) - basically the visualization of Prometheus. Cloud Migration​ No Candidate Workflow Engine​ Whirl [✓] In-house solution for defining and running workflows. Temporal [✓] A workflow engine used to orchestrate and ensure durable workflows across a distributed system. Rules Engine​ Grule Engine An open-source rule engine written in Golang. Messaging Bus​ NATS (Neural Autonomic Transport System) [✓]Apache Kafka Mapping​ Mapbox [✓] A powerful, feature-rich mapping service - it has mobile SDK's, front-end JavaScript SDK's and services that provide a lot of features such as navigation and geocoding. Self-hosted option available via Atlas. OpenLayers Leaflet GIS (Geographic Information System)​ Tile38 Geospatial Database &amp; Geofencing Server. TurfJS Simple geospatial analysis library. Digital Twin​ 3D Visualization​ ThreeJS [✓]Mapbox [✓]XeokitUnreal EngineUnity Process Simulation​ No candidate Phyiscal Simulation​ No candidate Optimization Engine​ Route, Resource and Other Model Optimization​ OR Tools Process Mining​ No candidate Universal Communications​ Chat, Audio and Video (Web RTC)​ Pion [✓] Email &amp; SMS​ No candidate Notifications​ Novu Web Component Framework​ Svelte [✓] Component framework for JavaScript - compiles code written in Svelte (similar to JSX) into pure JavaScript. React Web Application Framework​ Svelte Kit [✓] Web application framework with first-class support for Svelte - handles routing, server-side rendering, hybrid-rendering, client-side routing, etc. CSS / Styling​ TailwindCSS [✓] A CSS design system with extensive tooling. Drawing​ JointJS [✓] A powerful JavaScript diagramming library. Charts​ Apache ECharts [✓] A powerful open-source charting library. Iconography​ Font Awesome [✓] An extensive icon library with support for SVG's, layering and transformations, animations and is extensible. Graph QL​ urql [✓] Extensible GraphQL client with support for caching, retries, deduplication, subscriptions and more. User Experience​ Design &amp; Concept​ Figma [✓] De-facto standard for UI design and collaboration. Design System​ Bespoke, inspired by Material UI. We are currently lacking the resources to build and maintain a full-fledged design system. Please consider volunteering yourself (full-time) if you would like to participate. Anomaly Detection​ DA BU (Internal) AI/ML Engine​ DA BU (Internal) Fusion Engine​ DA BU (Internal) Templating Engine​ Explored and dropped Templating Language​ Explored and dropped Database​ SQL Non-distributed PostgreSQL [✓] Distributed YugabyteCockroachDB No-SQL Non-distributed MongoDB Distributed Apache CassandraMongoDB Testing​ Playwright [✓] End-to-end/integration testing framework. Allure Framework [✓] Reporting tool for tests. Documentation​ Docusaurus [✓] Open-source site generator based on markdown files. Open API Currently probably the only API specification standard that's widely adopted.","keywords":"","version":"Next"},{"title":"Playwright","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/playwright","content":"Playwright We use Playwright to automate testing on our web app. What is Playwright?​ Playwright is a framework for automating web end-to-end testing. It is built to enable cross-browser web automation, and is designed to run fast and reliably (with auto-waiting for tests, we don't have to insert artificial timeouts). Why we've chosen Playwright​ The main contenders to Playwright is Selenium and Cypress. Playwright is much newer compared to the other two, however, it has many advantages. Compared to Selenium, it creates much more reliable tests. For interaction with DOM elements, Selenium assumes that the element is going to be found already loaded on the page and if the element is not there, the test fails. Playwright however, has locators that have &quot;awaits&quot; natively, making execution of tests much more stable (yes,Cypress also has a similar feature). On top of that, Playwright supports Chromium, Firefox and Webkit (Safari).Cypress only supports Chromium and Firefox. The main advantange Cypress has over Playwright is that it is easy to set up and get-going and that it is more mature, however, Playwright is ultimately more powerful, and as the product matures, we also expect it to dominate the test market. Using Playwright​ Import test and expect to test using Playwright: import { test, expect } from '@playwright/test'; Write a test: test('Title Test', async ({ page }) =&gt; { await page.goto('http://localhost:3000'); const title = page.locator('title'); await expect(title).toHaveText('Home'); }); You can also use Playwright's test generator to generate the test code based on actions you perform directly on the browser - similar to Selenium's Record and Playback and Cypress Studio. npx playwright codegen http://localhost:3000 note You need to have the Playwright browsers installed, if you haven't already done so, run npx playwright install first. More Reading​ Official Playwright Website","keywords":"","version":"Next"},{"title":"Pino","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/pino","content":"Pino Pino is a very low overhead Node.js logger that we use to provide structured logging. What is Pino?​ Pino is a Node.js logger that uses minimum resources for logging. Why we've chosen Pino​ Using minimum resources for logging is very important. Log messages tend to get added over time and this can lead to a throttling effect on applications – such as reduced requests per second. Pino claims to be over 5x faster than alternatives. See the Benchmarks for comparisons. There are many logging libraries out there such as Winston and Bunyan. However, we chose Pino over them as it is very low overhead, and is has the ability to log on the browser, which is something Winston cannot do. We require our JavaScript code to be isomorphic as we use Svelte Kit (the same code is expected to run on both the browser and the server/Node.js) Using Pino​ We provide a configured Pino logger in the store that can be used across components. Import logger: import { logger } from '$lib/stores/Logger'; You can then use logger to call functions from Pino's API: logger.info({ Forecast: forecast.forecast }, 'Weather Forecast'); logger.error({ Error: err }, 'Weather API error'); More Reading​ Official Pino WebsiteAPIBrowser APIGitHub Repo","keywords":"","version":"Next"},{"title":"TypeScript","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/typescript","content":"TypeScript We use TypeScript on top of JavaScript to help our code be much more maintainable. Why TypeScript?​ TypeScript is a language for application-scale JavaScript. It adds optional types to JavaScript that support tools for large-scale JavaScript applications. The benefit of having types allows our code editors to have helpful code completion and type checking, and allows JavaScript to be actually maintainable. The goal is to have code that is more reliable, easier to read, and easier to refactor. More Reading​ Official TypeScript Website","keywords":"","version":"Next"},{"title":"Tailwind CSS","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/tailwind","content":"Tailwind CSS Tailwind is a utility-first CSS framework that we use for rapidly building custom user interfaces. What is Tailwind CSS?​ Tailwind is a utility CSS Framework with flexibility, optimization and heavy DX focus. It is a highly customizable, low-level CSS framework that gives you all the building blocks you need to build bespoke designs without opinionated styles you have to fight to override. Why we've chosen Tailwind​ When thinking about utility-class CSS Frameworks, two frameworks come to mind: Tailwind CSS and Bootstrap. While Bootstrap comes with a set of pre-styled responsive, components that make up a UI kit, Tailwind CSS offers a responsive design system that is more flexible and customizable. It also adds its powerfulstate variants system. Using Tailwind CSS​ Simply add the Tailwind CSS classes to your HTML elements to get going! &lt;h1 class=&quot;text-3xl font-bold underline&quot;&gt; Hello world! &lt;/h1&gt; You can also use arbitrary values, which basically works just like inline-styles. It is not recommended to break out of the constraints, but you may do so if it is really necessary (you should consider if you really have to whenever you decide to do this). &lt;div class=&quot;top-[117px]&quot;&gt; Hello world! &lt;/div&gt; Another powerful Tailwind feature to take note of is the ability toconditionally apply classes based on states. In this example, hovering over the &lt;div&gt; will apply the bg-gray-300 class. &lt;div class=&quot;bg-gray-200 hover:bg-gray-300&quot;&gt; Hello world! &lt;/div&gt; This feature even works with stylingbased on other related elements' state. Tailwind is highly extensible and has many more useful features. Read their official documentation for more info. More Reading​ Official Tailwind CSS WebsiteInstall Tailwind CSS with SvelteKitTailwind Core Concepts","keywords":"","version":"Next"},{"title":"Svelte & SvelteKit","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/svelte_sveltekit","content":"Svelte &amp; SvelteKit We've chosen Svelte as our primary component-based UI framework and SvelteKit as the web application framework (routing) for Svelte. What is Svelte?​ Svelte is a JavaScript framework for building user interfaces. It is component-based, much like the ever-popular React. However, unlike React, it does not create a virtual DOM and diff changes against it. Svelte shifts that work into thecompile step instead. Svelte's compiler can determine all the places where your code changes state -it does so by checking variable assignments. Why we've chosen Svelte​ The knee-jerk reaction you might have is why use Svelte when we already have React, a mature, popular framework with the backing of Facebook/Meta. Svelte is easier to pick up than React and is simply easy to learn in absolute terms. It's also feature-rich and highly performant - because it does not use a virtual DOM, Svelte is fast. Our choice to go with Svelte is also to get ahead of the curve. Trends are showing that Svelte is well-loved by the developer community and is constantly growing in popularity. Svelte was voted themost loved web frameworkin the Stack Overflows' 2021 survey won the highest satisfaction ratingsin State of JS's 2020 survey. With Rich Harris' induction into Vercel at the end of 2021, Svelte also now has the backing of a strong web-oriented tech-company and we believe it is a safe and forward-looking choice to use as our predominant framework. What is SvelteKit?​ SvelteKit is an application framework for building extremely high-performance web apps. Building an app with all the modern best practices is complicated. Those practices include build optimizations, so that you load only the minimal required code; offline support; prefetching pages before the user initiates navigation; and configurable rendering that allows you to generate HTML on the server or in the browser at runtime or at build-time. SvelteKit does all that for us. It uses Vite with a Svelte plugin to provide a lightning-fast and feature-rich development experience with Hot Module Replacement (HMR), where changes to your code are reflected in the browser instantly. Why we've chosen SvelteKit​ SvelteKit is the easiest way to get started with Svelte. It's the official web application framework for Svelteand is being developed closely in tandem with Svelte. Apart from that, it also has many best practices and optimizations built-in whilst providing a great developer experience with Vite's speedy Hot Module Replacement. caution Unfortunately, at the time of writing, SvelteKit is still in beta, and so breaking changes might occur now and then, which is a pain to deal with but we expect this issue to be resolved in the near future (as SvelteKit enters v1.0) More Reading​ Official Svelte WebsiteOfficial SvelteKit WebsiteOfficial Vite Website","keywords":"","version":"Next"},{"title":"🆕 Introduction","type":0,"sectionRef":"#","url":"/aoh-docs/docs/rnr/introduction","content":"🆕 Introduction RNR is a backend service which record &amp; replay past db events.","keywords":"","version":"Next"},{"title":"Record & Replay","type":0,"sectionRef":"#","url":"/aoh-docs/docs/rnr/Record & Replay API/record-replay","content":"Version: 1.0.0 Record &amp; Replay API for record &amp; replay service","keywords":"","version":"Next"},{"title":"urql","type":0,"sectionRef":"#","url":"/aoh-docs/docs/overview/technologies/urql","content":"urql urql is a highly customizable and versatile GraphQL client What is urql?​ urql is a GraphQL client built to be highly customisable and versatile and exposes a set of helpers for several frameworks. Why we've chosen urql​ When compared with other GraphQL clients such as Apollo and Relay, we chose urql because it has smallest base bundle size and is very lightweight and extensible. Though Apollo is a big name, Apollo Client also has many issues at the moment and requires more work to get working well with Svelte. Using urql​ Import necessary types and functions from urql: import type { Client } from '@urql/core'; import { createClient, defaultExchanges, subscriptionExchange } from '@urql/core'; Create the client: myClient = writable&lt;Client&gt;( createClient({ url: import.meta.env.VITE_GQL_HOST ? GQL_URL : 'http://your-graphql-endpoint.com/v1/graphql', fetchOptions: { headers: { 'content-type': 'application/json', 'Authorization': 'Bearer ' + jwt, }, }, exchanges: [ ...defaultExchanges, subscriptionExchange({ forwardSubscription: operation =&gt; ({ subscribe: sink =&gt; ({ unsubscribe: get(wsClient).subscribe(operation, sink), }), }), }), ], }) ); More Reading​ Official URQL Website","keywords":"","version":"Next"},{"title":"⚙️ Configuration","type":0,"sectionRef":"#","url":"/aoh-docs/docs/ucs/guides/configuration","content":"⚙️ Configuration All the configuration for the services are inside the .toml files. To run locally set the configuration inside VidConf/VidConf-ion/configs or if you want to deploy using docker set the configuration inside VidConf/VidConf-ion/configs/docker. SFU Configuration​ If browser cannot establish the ice canndidate pair with SFU, consider mapping external ip address to sfu.toml file. # if the sfu is deployed in a DMZ between two 1-1 NAT for internal and # external users. nat1to1 = [&quot;xxx.xxx.xxx.xxx&quot;] icelite = true Or use stun/turn server [[webrtc.iceserver]] urls = [&quot;stun:example:3478&quot;] [[webrtc.iceserver]] urls = [&quot;turn:turn.example.org:3478&quot;] username = &quot;username&quot; credential = &quot;password&quot; Or enable embeded sfu turn server [turn] # Enables embeded turn server enabled = true # Sets the realm for turn server realm = &quot;ion&quot; # The address the TURN server will listen on. address = &quot;0.0.0.0:3478&quot; # Certs path to config tls/dtls # cert=&quot;path/to/cert.pem&quot; # key=&quot;path/to/key.pem&quot; # Port range that turn relays to SFU # WARNING: It shouldn't overlap webrtc.portrange # Format: [min, max] # portrange = [5201, 5400] [turn.auth] # Use an auth secret to generate long-term credentials defined in RFC5389-10.2 # NOTE: This takes precedence over `credentials` if defined. # secret = &quot;secret&quot; # Sets the credentials pairs credentials = &quot;pion=ion,pion2=ion2&quot; info crendentials pair are separated by '='. pion=ion means username = pion and crendential = ion Browser Configuration​ If SFU is configured to be used with stun/turn server, please specific the stun/turn in brwoser configuration. const config = { iceServers: [ { urls: [&quot;turn:turn.example.org:3478&quot;], username: &quot;username&quot;, credential: &quot;password&quot;, }, ], }; ","keywords":"","version":"Next"},{"title":"💻 Development","type":0,"sectionRef":"#","url":"/aoh-docs/docs/ucs/guides/development","content":"💻 Development This is the documentation of frontend development for UCS backend specific features. For common usage of sdk, please refer to the official document ion-sdk-js. Message​ Message is the payload send through data channel. Message can be chat, incoming/outgoing call signal or file attachment. Please see mineType enumeration for different types of messages. Schema​ { &quot;uid&quot;: &quot;string&quot;, &quot;name&quot;: &quot;string&quot;, &quot;mime_type&quot;: &quot;string&quot;, &quot;text&quot;: &quot;string&quot;, &quot;attachment&quot;: { &quot;name&quot;: &quot;string&quot;, &quot;size&quot;: 0, &quot;file_path&quot;: &quot;string&quot; } } MineType Enumeration​ MineType determines the type of UCS communications. MineType\tDescriptionmessage\tChat message call-start\tStart video/audio call call-join\tJoin video/audio call call-end\tEnd video/audio call attachment\tShare file attachment broadcast-start\tStart sharing video broadcast-end\tStop sharing video disconnect\tDisconnect from the room Chat​ Set mineType to message and set the value of text in message schema. { &quot;uid&quot;: &quot;10206739&quot;, &quot;name&quot;: &quot;alex&quot;, &quot;mime_type&quot;: &quot;message&quot;, &quot;text&quot;: &quot;This is a test message&quot; } Audio/Video Call​ Examples of message payloads for call. Start Call​ { &quot;uid&quot;: &quot;10206739&quot;, &quot;name&quot;: &quot;alex&quot;, &quot;mime_type&quot;: &quot;call-start&quot; } Join Call​ { &quot;uid&quot;: &quot;10208888&quot;, &quot;name&quot;: &quot;bob&quot;, &quot;mime_type&quot;: &quot;call-join&quot; } End Call​ { &quot;uid&quot;: &quot;10208888&quot;, &quot;name&quot;: &quot;bob&quot;, &quot;mime_type&quot;: &quot;call-end&quot; } File Sharing​ There is a size limitation in WebRTC data channel which only accepts up to 16MB max. We used MinIO presigned Url to upload/download file attachment. For more details on MinIO, please refer to their official documentation. File_path is the location of MinIO object. Set this to the value received from the room mannagment API. { &quot;uid&quot;: &quot;10206739&quot;, &quot;name&quot;: &quot;alex&quot;, &quot;mime_type&quot;: &quot;attachment&quot;, &quot;attachment&quot;: { &quot;name&quot;: &quot;testFile.txt&quot;, &quot;size&quot;: 100, &quot;file_path&quot;: &quot;/attachment/19649298-b4da-4f2f-9e24-c7ebdc20a766&quot; } } ","keywords":"","version":"Next"},{"title":"🌎 Deployment","type":0,"sectionRef":"#","url":"/aoh-docs/docs/ucs/guides/deployment","content":"🌎 Deployment Building Images​ Building all services in docker-compose file. docker-compose build Building a specifc service. docker-compose build {service-name} danger It is important to build the image following below naming convention. Otherwise, pushing to ghcr will fail. ghcr.io/NAMESPACE/IMAGE_NAME:tag NAMESPACE must be personal account or organiztion to which the image will be scoped to. Pushing to Container Registry​ This project used ghcr (github container registry) to store images. Working with ghcr create a new github personal access token with at least write:pacakges access.login to ghcr using cli. export CR_PAT=YOUR_TOKEN echo $CR_PAT | docker login ghcr.io -u USERNAME --password-stdin push docker push ghcr.io/NAMESPACE/IMAGE_NAME:latest For more details, refer to github offical documents Deploy UCS in Docker Container​ docker-compose up -d --remove-orphans info UCS services use both .env and .toml. .env is used for passing secret keys and .toml is for configuration. Configuring docker-compose file. Set env variable environment: - MINIO_ACCESS_KEY=${MINIO_ACCESS_KEY} - MINIO_SECRET_KEY=${MINIO_SECRET_KEY} - POSTGRESQL_USER=${POSTGRESQL_USER} - POSTGRESQL_PASSWORD=${POSTGRESQL_PASSWORD} Set toml config file location volumes: - &quot;./configs/docker/app-room-mgmt.toml:/configs/app-room-mgmt.toml&quot; Deploy UCS in kubernetes​ Preparing Manifest Repo​ Create a new repository with below directory for UCS services. Create three yaml files for each UCS service. . └── manifests ├── {service-name}-config.yaml ├── {service-name}-deployment.yaml ├── {service-name}-service.yaml For detail explanation of yaml files, refer to official documents for: deployment.yamlservice.yamlconfig.yaml You can use kompose to easily convert from docker-compose.yaml into deployment.yaml &amp; service.yaml. kompose convert -f docker-compose.yaml config.yaml is to load ucs config .toml into deployment. Example config.yaml. apiVersion: v1 kind: ConfigMap metadata: creationTimestamp: null name: app-room-mgmt-config namespace: common-ucs data: app-room-mgmt.toml: | [log] level = &quot;info&quot; ... And mount the config.yaml in deployment.yaml volumes. apiVersion: apps/v1 kind: Deployment spec: template: spec: volumes: - name: app-room-mgmt-claim0 configMap: name: app-room-mgmt-config Deploy Using ArgoCD​ If ArgoCD is setup for kubernetes deployment, you just have to create entrypoint for ArgoCD to find the manifest repo. Create a yaml inside the repo where ArgoCD will look for entrypoint. Make sure to provide the repoURL and path of minifest repo. apiVersion: argoproj.io/v1alpha1 kind: Application metadata: name: ucs namespace: argocd finalizers: - resources-finalizer.argocd.argoproj.io spec: destination: namespace: common-ucs name: in-cluster project: appbundle-project-ar2-dev source: path: manifests repoURL: https://github.com/example/ucs targetRevision: main syncPolicy: syncOptions: - CreateNamespace=true automated: prune: true allowEmpty: true selfHeal: true Check if the Deployment was created using ArgoCD dashboard. Deploy Using kubectl​ Before you begin, make sure your Kubernetes cluster is up and running. Follow the steps given below to create the above Deployment: Create the Deployment by running the following command: kubectl apply -f https://github.com/example/ucs/manifest/app-room-mgmt-deployment.yaml Run kubectl get deployments to check if the Deployment was created. CI/CD Pipeline​ Every commit to main branch will trigger the github action workflow. Following actions will be triggered by workflow - build &amp; push the images into ghcr (github container registry)update the image tags in manifest files of staging server Upon the update of mainfest files, ArgoCD will pickup new images' tag to deploy latest servces. Refer to publish.yml inside .github/workflows for more details.","keywords":"","version":"Next"},{"title":"🚀 Quickstart","type":0,"sectionRef":"#","url":"/aoh-docs/docs/ucs/guides/quickstart","content":"🚀 Quickstart Pre-requisites​ Required: DockerK3s Running Locally​ For configuration of the forked services, please refer to official documentations from the referenced repositories:- ION - ION backend services: SFU, signal, app-room, islbION-app-web Docker setup:-​ Install DockerInstall Docker-ComposeDocker Post-installation setupClone git clone https://github.com/mssfoobar/VidConf.git Change &quot;localhost&quot; to hostname or domain name in file VidConf/VidConf-ion-app-web/CaddyfileUpdate the postgresql/minio/systemUserId configuration for the following:- ./VidConf-ion/configs/docker/app-room.toml./VidConf-ion/configs/docker/app-room-mgmt.toml./VidConf-ion/configs/docker/app-room-sentry.toml./VidConf-ion/configs/docker/app-room-recorder.toml./VidConf-ion/configs/docker/app-room-playback.toml Setup cd VidConf/ ./scripts/setupDocker.sh VerifyChat: https://localhost:9090Stop containers ./scripts/stopDocker.sh Start containers ./scripts/startDocker.sh Cleanup Docker ./scripts/cleanupDocker.sh K3s setup:-​ Install K3sInstall krelay for UDP port forwardingClone project git clone https://github.com/mssfoobar/VidConf.git Configure IP address - Update local IP address inside file: web-caddy-file---configmap.yamlSetup ./scripts/setupK3s.sh k3s kubectl relay --address 0.0.0.0 deployment/sfu 5000:5000 k3s kubectl port-forward deployment/signal 5551:5551 --address='0.0.0.0' k3s kubectl port-forward deployment/sfu 3478:3478 --address='0.0.0.0' k3s kubectl port-forward deployment/nats 4222:4222 --address='0.0.0.0' k3s kubectl port-forward deployment/redis 6379:6379 --address='0.0.0.0' k3s kubectl port-forward deployment/web 9090:9090 --address='0.0.0.0' Verify Chat: http://local_ip_address:9090","keywords":"","version":"Next"},{"title":"Nats","type":0,"sectionRef":"#","url":"/aoh-docs/docs/ucs/guides/nats","content":"Nats We used nats subject-based messaging within UCS services. Below is the list of nats subject in use. UPDATEROOM_TOPIC string = &quot;roomUpdates.{sid}&quot; STARTRECORDING_TOPIC string = &quot;recordingStart&quot; ENDRECORDING_TOPIC string = &quot;recordingEnd.{sid}&quot; STARTPLAYBACK_TOPIC string = &quot;playbackStart&quot; ENDPLAYBACK_TOPIC string = &quot;playbackEnd.{sid}&quot; DELETEPLAYBACK_TOPIC string = &quot;playbackDelete.{sid}&quot; PAUSEPLAYBACK_TOPIC string = &quot;playbackPause.{sid}&quot; PLAYBACK_TOPIC string = &quot;playback.{sid}&quot; INSERT_CALLUSERID_TOPIC string = &quot;insertCallUserId.{sid}&quot; DELETE_CALLUSERID_TOPIC string = &quot;deleteCallUserId.{sid}&quot; INSERT_CHATUSERID_TOPIC string = &quot;insertChatUserId.{sid}&quot; DELETE_CHATUSERID_TOPIC string = &quot;deleteChatUserId.{sid}&quot; ","keywords":"","version":"Next"},{"title":"🆕 Introduction","type":0,"sectionRef":"#","url":"/aoh-docs/docs/ucs/introduction","content":"🆕 Introduction UCS is a real-time communication system developed for AOH projects. UCS repositories can be found here. https://github.com/mssfoobar/Vidconf-ion.githttps://github.com/mssfoobar/VidConf-ion-app-web.git Supported Features:​ Containerised deployment with DevOps consideration in placeWeb-based frontend for ease of deploymentVideo conference to share video, audio, chat messages and files primarily for team-sized collaborations in the course of fulfilling daily tasksHTTPS-API for service provisioning, e.g. advance-room-bookingRecording and Playback of conference sessions to record-and-playback the data streams instead of screen capturing of conference sessionsData-encryption for all network trafficWeb-security to disable unencrpyted data through well-known portsScalable backend architectureHigh-availability architectureMax participant around 200 paxScreen-sharing for meetingsSystem-monitoring for health statusPartial-disabling of video, audio or chatISO8601 zulu timeRoomId is google UUIDQuery params for HTTP POST is inside JSON bodyKick user by userIdQuery Room by roomIdBookroom: optional prompt RelativeFrom:Start/End(Default End) RelativeTimeInSeconds:123 Message:PromptMessageEdit room bookingCancel room booking Forked Projects:​ UCS is forked from the following open-source projects:- ION - a complete WebRTC-compatible backend suite written in golangION-app-web - a WebRTC-compatible web-frontend written in golang","keywords":"","version":"Next"},{"title":"🔧 Testing","type":0,"sectionRef":"#","url":"/aoh-docs/docs/ucs/guides/testing","content":"🔧 Testing SFU load testing tool​ Clone the vidconf-loadtest repository git clone https://github.com/mssfoobar/vidconf-loadtest.git Test File​ Publishing of files in the following formats are supported. Container\tVideo Codecs\tAudioWEBM\tVP8\tOPUS If your data is not webm, you can use ffmpeg to make one This show how to make a 0.5Mbps webm: ffmpeg -i djrm480p.mp4 -strict -2 -b:v 0.4M -vcodec libvpx -acodec opus djrm480p.webm See the ffmpeg docs on VP8 for encoding options Quick Start​ Change necessary Makefile variables # number of client publishing the video stream pubClient = 1 # number of client subscribing the video stream subClient = 10 # signal address signalAddr = 127.0.0.1:5551 # session id sessionId = ion # duration the load testing tool will run duration = 600 Use makefile commands to start the tool in docker make build make start Run tool in another Linux server​ # build a linux version, we test on linux because mac fd limit env GOOS=linux go build -o sfu-loadtest ./sfu-loadtest/main.go # pub.sh #./sfu-loadtest -file ./djrm480p.webm -clients 1 -role pubsub -gaddr &quot;yoursfuip:5551&quot; -session 'ion' -log debug -cycle 1000 -a -v # sub.sh #../sfu-loadtest -file /Volumes/vm/media/djrm480p.webm -clients 10 -role sub -gaddr &quot;127.0.0.1:5551&quot; -session 'ion' ","keywords":"","version":"Next"},{"title":"Room Management","type":0,"sectionRef":"#","url":"/aoh-docs/docs/ucs/Room Management API/room-management","content":"Version: 1.0.0 Room Management API for room management HTTP server","keywords":"","version":"Next"},{"title":"Geo-Entities","type":0,"sectionRef":"#","url":"/aoh-docs/docs/gis/geoentity","content":"Geo-Entities AGIL Ops Hub provides a standardized format for storing geographic data structures. The primary purpose of this is to keep the information required to efficiently render entities on the map. Schema Geographic Entity (Base Interface)​ { &quot;$ref&quot;: &quot;#/definitions/GeoEntity&quot;, &quot;$schema&quot;: &quot;http://json-schema.org/draft-07/schema#&quot;, &quot;definitions&quot;: { &quot;BBox&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;maxItems&quot;: 4, &quot;minItems&quot;: 4, &quot;type&quot;: &quot;array&quot; }, { &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;maxItems&quot;: 6, &quot;minItems&quot;: 6, &quot;type&quot;: &quot;array&quot; } ], &quot;description&quot;: &quot;Bounding box https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;GeoEntity&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;data&quot;: { &quot;$ref&quot;: &quot;#/definitions/GeoEntityData&quot;, &quot;description&quot;: &quot;Data properties are dependent on the entity type&quot; }, &quot;entity_type&quot;: { &quot;$ref&quot;: &quot;#/definitions/GeoEntityType&quot;, &quot;description&quot;: &quot;The type of the geoentity, which will affect the data's properties&quot; }, &quot;id&quot;: { &quot;description&quot;: &quot;A uuid for the data (should be unique across all AOH GIS Geo Entities)&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;version&quot;: { &quot;description&quot;: &quot;The semver of the geo entity schema that this object complies with&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;id&quot;, &quot;entity_type&quot;, &quot;version&quot;, &quot;data&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;GeoEntityData&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;GeoEntityData is the base interface for each entity type. The schema for the data is dependent on which entity type this GeoEntity is.&quot;, &quot;properties&quot;: { &quot;meta_geo&quot;: { &quot;$ref&quot;: &quot;#/definitions/Point&quot;, &quot;description&quot;: &quot;The GeoJSON point here may a `bbox` that encompasses all the features within this GeoEntityData. The purpose of this is to facilitate efficient querying of whether the geo entity would be within a certain bounds (for the purposes of rendering or calculations).&quot; } }, &quot;required&quot;: [ &quot;meta_geo&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;GeoEntityType&quot;: { &quot;description&quot;: &quot;Depending on the geoentity type, certain properties or attributes will be different in data and GeoJSON features and this will affect how they are rendered.\\n\\nThere are plans to add a \\&quot;path\\&quot; type that allows indoor routing but this is pending technical feasibility analysis with Mapbox Atlas's features&quot;, &quot;enum&quot;: [ &quot;building&quot;, &quot;floor&quot;, &quot;track&quot;, &quot;annotation&quot; ], &quot;type&quot;: &quot;string&quot; }, &quot;Point&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Point geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.2&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Point&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Position&quot;: { &quot;description&quot;: &quot;A Position is an array of coordinates. https://tools.ietf.org/html/rfc7946#section-3.1.1 Array should contain between two and three elements. The previous GeoJSON specification allowed more elements (e.g., which could be used to represent M values), but the current specification only allows X, Y, and (optionally) Z to be defined.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;type&quot;: &quot;array&quot; } } } Building​ Building data is separated from its floor data. This is done for performance reasons. Floors may contain a large amount of information, so they are treated as separate entities and contain a reference to the building they are meant to be a part of. The main purpose of this Building geographic entity is to hold information describing the building (such as its name, and where it sits etc.) { &quot;$ref&quot;: &quot;#/definitions/Building&quot;, &quot;$schema&quot;: &quot;http://json-schema.org/draft-07/schema#&quot;, &quot;definitions&quot;: { &quot;AddressablePOIProperties&quot;: { &quot;additionalProperties&quot;: false, &quot;properties&quot;: { &quot;description&quot;: { &quot;description&quot;: &quot;A useful description fo&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;full_address&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;icon&quot;: { &quot;description&quot;: &quot;The name of the icon that appears over the point&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;image&quot;: { &quot;description&quot;: &quot;A URL to the image of the location&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;label&quot;: { &quot;description&quot;: &quot;An informative label used for display - this could be the name or title of the place of interest&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;tags&quot;: { &quot;description&quot;: &quot;Useful for searching/filtering&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; } }, &quot;required&quot;: [ &quot;icon&quot;, &quot;label&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;AgisBuildingData&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Building data contains the information to render buildings on the map as well as their POI information&quot;, &quot;properties&quot;: { &quot;building_id&quot;: { &quot;description&quot;: &quot;Identifier unique across all AOH GIS buildings&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;form_features&quot;: { &quot;$ref&quot;: &quot;#/definitions/BuildingFormFeatureCollection&quot; }, &quot;full_address&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;meta_geo&quot;: { &quot;$ref&quot;: &quot;#/definitions/Point&quot;, &quot;description&quot;: &quot;The GeoJSON point here may a `bbox` that encompasses all the features within this GeoEntityData. The purpose of this is to facilitate efficient querying of whether the geo entity would be within a certain bounds (for the purposes of rendering or calculations).&quot; }, &quot;poi_feature&quot;: { &quot;$ref&quot;: &quot;#/definitions/PointOfInterestFeature&quot; } }, &quot;required&quot;: [ &quot;building_id&quot;, &quot;form_features&quot;, &quot;full_address&quot;, &quot;meta_geo&quot;, &quot;poi_feature&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;BBox&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;maxItems&quot;: 4, &quot;minItems&quot;: 4, &quot;type&quot;: &quot;array&quot; }, { &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;maxItems&quot;: 6, &quot;minItems&quot;: 6, &quot;type&quot;: &quot;array&quot; } ], &quot;description&quot;: &quot;Bounding box https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;Building&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;The 'Buildng' type used in AGIL Ops Hub is meant to support displaying them as 3-dimensional extrusions in Mapbox. The data provided is meant to support rendering a building in this manner.&quot;, &quot;properties&quot;: { &quot;data&quot;: { &quot;$ref&quot;: &quot;#/definitions/AgisBuildingData&quot;, &quot;description&quot;: &quot;Data properties are dependent on the entity type&quot; }, &quot;entity_type&quot;: { &quot;const&quot;: &quot;building&quot;, &quot;description&quot;: &quot;The type of the geoentity, which will affect the data's properties&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;id&quot;: { &quot;description&quot;: &quot;A uuid for the data (should be unique across all AOH GIS Geo Entities)&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;version&quot;: { &quot;description&quot;: &quot;The semver of the geo entity schema that this object complies with&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;data&quot;, &quot;entity_type&quot;, &quot;id&quot;, &quot;version&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;BuildingFormFeatureCollection&quot;: { &quot;$ref&quot;: &quot;#/definitions/FeatureCollection%3CMultiPolygon%2CBuildingFormFeatureProperties%3E&quot;, &quot;description&quot;: &quot;The building form refers to the exterior visual shape of the building - this is used to render the 3D representation of the building exterior.&quot; }, &quot;BuildingFormFeatureProperties&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Building form feature properties are to describe how the building form feature can be rendered on the map&quot;, &quot;properties&quot;: { &quot;height&quot;: { &quot;description&quot;: &quot;The height from the ground/base level to the top of the feature in the floor&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;height_to_base&quot;: { &quot;description&quot;: &quot;the height from ground/base level to the bottom of the feature in the floor&quot;, &quot;type&quot;: &quot;number&quot; } }, &quot;required&quot;: [ &quot;height&quot;, &quot;height_to_base&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;BusinessPOIProperties&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;A business point of interest would typically be a shop selling some service, like a restaurant.&quot;, &quot;properties&quot;: { &quot;description&quot;: { &quot;description&quot;: &quot;A useful description fo&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;full_address&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;icon&quot;: { &quot;description&quot;: &quot;The name of the icon that appears over the point&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;image&quot;: { &quot;description&quot;: &quot;A URL to the image of the location&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;label&quot;: { &quot;description&quot;: &quot;An informative label used for display - this could be the name or title of the place of interest&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;operating_hours&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;phone_number&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;tags&quot;: { &quot;description&quot;: &quot;Useful for searching/filtering&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;website&quot;: { &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;icon&quot;, &quot;label&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Feature&lt;MultiPolygon,BuildingFormFeatureProperties&gt;&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;A feature object which contains a geometry and associated properties. https://tools.ietf.org/html/rfc7946#section-3.2&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;geometry&quot;: { &quot;$ref&quot;: &quot;#/definitions/MultiPolygon&quot;, &quot;description&quot;: &quot;The feature's geometry&quot; }, &quot;id&quot;: { &quot;description&quot;: &quot;A value that uniquely identifies this feature in a https://tools.ietf.org/html/rfc7946#section-3.2.&quot;, &quot;type&quot;: [ &quot;string&quot;, &quot;number&quot; ] }, &quot;properties&quot;: { &quot;$ref&quot;: &quot;#/definitions/BuildingFormFeatureProperties&quot;, &quot;description&quot;: &quot;Properties associated with this feature.&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Feature&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;geometry&quot;, &quot;properties&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Feature&lt;Point,(PointOfInterestFeatureProperties|AddressablePOIProperties|BusinessPOIProperties)&gt;&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;A feature object which contains a geometry and associated properties. https://tools.ietf.org/html/rfc7946#section-3.2&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;geometry&quot;: { &quot;$ref&quot;: &quot;#/definitions/Point&quot;, &quot;description&quot;: &quot;The feature's geometry&quot; }, &quot;id&quot;: { &quot;description&quot;: &quot;A value that uniquely identifies this feature in a https://tools.ietf.org/html/rfc7946#section-3.2.&quot;, &quot;type&quot;: [ &quot;string&quot;, &quot;number&quot; ] }, &quot;properties&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/definitions/PointOfInterestFeatureProperties&quot; }, { &quot;$ref&quot;: &quot;#/definitions/AddressablePOIProperties&quot; }, { &quot;$ref&quot;: &quot;#/definitions/BusinessPOIProperties&quot; } ], &quot;description&quot;: &quot;Properties associated with this feature.&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Feature&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;geometry&quot;, &quot;properties&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;FeatureCollection&lt;MultiPolygon,BuildingFormFeatureProperties&gt;&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;A collection of feature objects. https://tools.ietf.org/html/rfc7946#section-3.3&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;features&quot;: { &quot;items&quot;: { &quot;$ref&quot;: &quot;#/definitions/Feature%3CMultiPolygon%2CBuildingFormFeatureProperties%3E&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;FeatureCollection&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;features&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;MultiPolygon&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;MultiPolygon geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.7&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;items&quot;: { &quot;items&quot;: { &quot;items&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;MultiPolygon&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Point&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Point geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.2&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Point&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;PointOfInterestFeature&quot;: { &quot;$ref&quot;: &quot;#/definitions/Feature%3CPoint%2C(PointOfInterestFeatureProperties%7CAddressablePOIProperties%7CBusinessPOIProperties)%3E&quot;, &quot;description&quot;: &quot;A point of interest contains information used to show details about a point on the map - for example, an AED device location would be suitable to be marked as a point of interest&quot; }, &quot;PointOfInterestFeatureProperties&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Any entity that contains the relevant attributes for a place of interest can be displayed&quot;, &quot;properties&quot;: { &quot;description&quot;: { &quot;description&quot;: &quot;A useful description fo&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;icon&quot;: { &quot;description&quot;: &quot;The name of the icon that appears over the point&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;image&quot;: { &quot;description&quot;: &quot;A URL to the image of the location&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;label&quot;: { &quot;description&quot;: &quot;An informative label used for display - this could be the name or title of the place of interest&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;tags&quot;: { &quot;description&quot;: &quot;Useful for searching/filtering&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;string&quot; }, &quot;type&quot;: &quot;array&quot; } }, &quot;required&quot;: [ &quot;label&quot;, &quot;icon&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Position&quot;: { &quot;description&quot;: &quot;A Position is an array of coordinates. https://tools.ietf.org/html/rfc7946#section-3.1.1 Array should contain between two and three elements. The previous GeoJSON specification allowed more elements (e.g., which could be used to represent M values), but the current specification only allows X, Y, and (optionally) Z to be defined.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;type&quot;: &quot;array&quot; } } } Floor​ A floor in a building. { &quot;$ref&quot;: &quot;#/definitions/Floor&quot;, &quot;$schema&quot;: &quot;http://json-schema.org/draft-07/schema#&quot;, &quot;definitions&quot;: { &quot;AgisFloorData&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Floor data consists of feature collections of the entire floor.&quot;, &quot;properties&quot;: { &quot;building_id&quot;: { &quot;description&quot;: &quot;The id of building that this floor is a part of&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;features&quot;: { &quot;$ref&quot;: &quot;#/definitions/FloorFeatureCollection&quot;, &quot;description&quot;: &quot;The list of floor features for rendering&quot; }, &quot;floor&quot;: { &quot;description&quot;: &quot;The index of the level/floor/storey that this floor is in the build, starting at 0. Use negative numbers for underground levels (e.g. basement one should be -1)&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;floor_id&quot;: { &quot;description&quot;: &quot;Identifier unique across all AOH GIS floors&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;label&quot;: { &quot;description&quot;: &quot;A name or title to give to the floor for display (e.g. Basement 1 or Mezzanine)&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;meta_geo&quot;: { &quot;$ref&quot;: &quot;#/definitions/Point&quot;, &quot;description&quot;: &quot;The GeoJSON point here may a `bbox` that encompasses all the features within this GeoEntityData. The purpose of this is to facilitate efficient querying of whether the geo entity would be within a certain bounds (for the purposes of rendering or calculations).&quot; } }, &quot;required&quot;: [ &quot;building_id&quot;, &quot;features&quot;, &quot;floor&quot;, &quot;floor_id&quot;, &quot;meta_geo&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;BBox&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;maxItems&quot;: 4, &quot;minItems&quot;: 4, &quot;type&quot;: &quot;array&quot; }, { &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;maxItems&quot;: 6, &quot;minItems&quot;: 6, &quot;type&quot;: &quot;array&quot; } ], &quot;description&quot;: &quot;Bounding box https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;Feature&lt;Geometry,FloorFeatureProperties&gt;&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;A feature object which contains a geometry and associated properties. https://tools.ietf.org/html/rfc7946#section-3.2&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;geometry&quot;: { &quot;$ref&quot;: &quot;#/definitions/Geometry&quot;, &quot;description&quot;: &quot;The feature's geometry&quot; }, &quot;id&quot;: { &quot;description&quot;: &quot;A value that uniquely identifies this feature in a https://tools.ietf.org/html/rfc7946#section-3.2.&quot;, &quot;type&quot;: [ &quot;string&quot;, &quot;number&quot; ] }, &quot;properties&quot;: { &quot;$ref&quot;: &quot;#/definitions/FloorFeatureProperties&quot;, &quot;description&quot;: &quot;Properties associated with this feature.&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Feature&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;geometry&quot;, &quot;properties&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;FeatureCollection&lt;Geometry,FloorFeatureProperties&gt;&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;A collection of feature objects. https://tools.ietf.org/html/rfc7946#section-3.3&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;features&quot;: { &quot;items&quot;: { &quot;$ref&quot;: &quot;#/definitions/Feature%3CGeometry%2CFloorFeatureProperties%3E&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;FeatureCollection&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;features&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Floor&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Floors are part of buildings and should not exist independently. They must be able to get a reference to the buildings they are a part of via the `building_id`. They contain a collection of features that's used to render individual floors in a building.&quot;, &quot;properties&quot;: { &quot;data&quot;: { &quot;$ref&quot;: &quot;#/definitions/AgisFloorData&quot;, &quot;description&quot;: &quot;Data properties are dependent on the entity type&quot; }, &quot;entity_type&quot;: { &quot;const&quot;: &quot;floor&quot;, &quot;description&quot;: &quot;The type of the geoentity, which will affect the data's properties&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;id&quot;: { &quot;description&quot;: &quot;A uuid for the data (should be unique across all AOH GIS Geo Entities)&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;version&quot;: { &quot;description&quot;: &quot;The semver of the geo entity schema that this object complies with&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;data&quot;, &quot;entity_type&quot;, &quot;id&quot;, &quot;version&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;FloorFeatureCollection&quot;: { &quot;$ref&quot;: &quot;#/definitions/FeatureCollection%3CGeometry%2CFloorFeatureProperties%3E&quot;, &quot;description&quot;: &quot;The GeoJSON feature collection of features in a floor.&quot; }, &quot;FloorFeatureProperties&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Properties used to determine how a floor feature is rendered. Height is required as not every item in a level is of the same height (e.g. counters can be rendered effectively this way)&quot;, &quot;properties&quot;: { &quot;height&quot;: { &quot;description&quot;: &quot;The height from the ground/base level to the top of the feature in the floor&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;height_to_base&quot;: { &quot;description&quot;: &quot;the height from ground/base level to the bottom of the feature in the floor&quot;, &quot;type&quot;: &quot;number&quot; }, &quot;type&quot;: { &quot;$ref&quot;: &quot;#/definitions/FloorFeatureType&quot;, &quot;description&quot;: &quot;Different floor feature types might be rendered differently&quot; } }, &quot;required&quot;: [ &quot;type&quot;, &quot;height&quot;, &quot;height_to_base&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;FloorFeatureType&quot;: { &quot;description&quot;: &quot;Each of these features might be mundane items like a wall, or it might be a lift or staircase that connects to other levels in the building. How they are rendered can be based on their type.&quot;, &quot;enum&quot;: [ &quot;wall&quot;, &quot;window&quot;, &quot;toilet&quot;, &quot;stairs&quot;, &quot;escalator&quot;, &quot;lift&quot;, &quot;unknown&quot; ], &quot;type&quot;: &quot;string&quot; }, &quot;Geometry&quot;: { &quot;anyOf&quot;: [ { &quot;$ref&quot;: &quot;#/definitions/Point&quot; }, { &quot;$ref&quot;: &quot;#/definitions/MultiPoint&quot; }, { &quot;$ref&quot;: &quot;#/definitions/LineString&quot; }, { &quot;$ref&quot;: &quot;#/definitions/MultiLineString&quot; }, { &quot;$ref&quot;: &quot;#/definitions/Polygon&quot; }, { &quot;$ref&quot;: &quot;#/definitions/MultiPolygon&quot; }, { &quot;$ref&quot;: &quot;#/definitions/GeometryCollection&quot; } ], &quot;description&quot;: &quot;Geometry object. https://tools.ietf.org/html/rfc7946#section-3&quot; }, &quot;GeometryCollection&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Geometry Collection https://tools.ietf.org/html/rfc7946#section-3.1.8&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;geometries&quot;: { &quot;items&quot;: { &quot;$ref&quot;: &quot;#/definitions/Geometry&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;GeometryCollection&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;geometries&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;LineString&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;LineString geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.4&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;items&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;LineString&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;MultiLineString&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;MultiLineString geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.5&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;items&quot;: { &quot;items&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;MultiLineString&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;MultiPoint&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;MultiPoint geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.3&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;items&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;MultiPoint&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;MultiPolygon&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;MultiPolygon geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.7&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;items&quot;: { &quot;items&quot;: { &quot;items&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;MultiPolygon&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Point&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Point geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.2&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Point&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Polygon&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Polygon geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.6&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;items&quot;: { &quot;items&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Polygon&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Position&quot;: { &quot;description&quot;: &quot;A Position is an array of coordinates. https://tools.ietf.org/html/rfc7946#section-3.1.1 Array should contain between two and three elements. The previous GeoJSON specification allowed more elements (e.g., which could be used to represent M values), but the current specification only allows X, Y, and (optionally) Z to be defined.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;type&quot;: &quot;array&quot; } } } Track​ Tracks are moving entities that are typically 'tracked' via telemetry data. These could be delivery vans, airplanes, or robots in a factory floor. { &quot;$ref&quot;: &quot;#/definitions/Track&quot;, &quot;$schema&quot;: &quot;http://json-schema.org/draft-07/schema#&quot;, &quot;definitions&quot;: { &quot;AgisTrackData&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Tracks only really need a location and an identifier, additional information should be retrieved when the track is queried (e.g. clicked on)&quot;, &quot;properties&quot;: { &quot;meta_geo&quot;: { &quot;$ref&quot;: &quot;#/definitions/Point&quot;, &quot;description&quot;: &quot;The GeoJSON point here may a `bbox` that encompasses all the features within this GeoEntityData. The purpose of this is to facilitate efficient querying of whether the geo entity would be within a certain bounds (for the purposes of rendering or calculations).&quot; }, &quot;track_feature&quot;: { &quot;$ref&quot;: &quot;#/definitions/TrackFeature&quot;, &quot;description&quot;: &quot;The information to display a track on the page. Tracks are by default expected to be points, any additional information about the track should be pulled separately using the `track_id`&quot; }, &quot;track_id&quot;: { &quot;description&quot;: &quot;Identifier unique across all AOH GIS tracks&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;meta_geo&quot;, &quot;track_feature&quot;, &quot;track_id&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;BBox&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;maxItems&quot;: 4, &quot;minItems&quot;: 4, &quot;type&quot;: &quot;array&quot; }, { &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;maxItems&quot;: 6, &quot;minItems&quot;: 6, &quot;type&quot;: &quot;array&quot; } ], &quot;description&quot;: &quot;Bounding box https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;Feature&lt;Point,TrackFeatureProperties&gt;&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;A feature object which contains a geometry and associated properties. https://tools.ietf.org/html/rfc7946#section-3.2&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;geometry&quot;: { &quot;$ref&quot;: &quot;#/definitions/Point&quot;, &quot;description&quot;: &quot;The feature's geometry&quot; }, &quot;id&quot;: { &quot;description&quot;: &quot;A value that uniquely identifies this feature in a https://tools.ietf.org/html/rfc7946#section-3.2.&quot;, &quot;type&quot;: [ &quot;string&quot;, &quot;number&quot; ] }, &quot;properties&quot;: { &quot;$ref&quot;: &quot;#/definitions/TrackFeatureProperties&quot;, &quot;description&quot;: &quot;Properties associated with this feature.&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Feature&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;geometry&quot;, &quot;properties&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Point&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Point geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.2&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Point&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Position&quot;: { &quot;description&quot;: &quot;A Position is an array of coordinates. https://tools.ietf.org/html/rfc7946#section-3.1.1 Array should contain between two and three elements. The previous GeoJSON specification allowed more elements (e.g., which could be used to represent M values), but the current specification only allows X, Y, and (optionally) Z to be defined.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;Track&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Dynamic entities that move frequently, such as transport vehicles with GPS tracking.&quot;, &quot;properties&quot;: { &quot;data&quot;: { &quot;$ref&quot;: &quot;#/definitions/AgisTrackData&quot;, &quot;description&quot;: &quot;Data properties are dependent on the entity type&quot; }, &quot;entity_type&quot;: { &quot;const&quot;: &quot;track&quot;, &quot;description&quot;: &quot;The type of the geoentity, which will affect the data's properties&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;id&quot;: { &quot;description&quot;: &quot;A uuid for the data (should be unique across all AOH GIS Geo Entities)&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;version&quot;: { &quot;description&quot;: &quot;The semver of the geo entity schema that this object complies with&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;data&quot;, &quot;entity_type&quot;, &quot;id&quot;, &quot;version&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;TrackFeature&quot;: { &quot;$ref&quot;: &quot;#/definitions/Feature%3CPoint%2CTrackFeatureProperties%3E&quot;, &quot;description&quot;: &quot;Track's would usually be rendered as points but may have additional information to render them differently&quot; }, &quot;TrackFeatureProperties&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Track feature properties are reserved for future expansion&quot;, &quot;properties&quot;: { &quot;icon&quot;: { &quot;description&quot;: &quot;The name of the icon that appears over the point&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;icon&quot; ], &quot;type&quot;: &quot;object&quot; } } } Annotation​ Overlay on a map. { &quot;$ref&quot;: &quot;#/definitions/Annotation&quot;, &quot;$schema&quot;: &quot;http://json-schema.org/draft-07/schema#&quot;, &quot;definitions&quot;: { &quot;AgisAnnotationData&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Annotations data contains annotation rendering information, as well as references to who owns these annotations.&quot;, &quot;properties&quot;: { &quot;annotation_id&quot;: { &quot;description&quot;: &quot;Identifier unique across all AOH GIS annotations&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;features&quot;: { &quot;$ref&quot;: &quot;#/definitions/AnnotationFeatureCollection&quot; }, &quot;meta_geo&quot;: { &quot;$ref&quot;: &quot;#/definitions/Point&quot;, &quot;description&quot;: &quot;The GeoJSON point here may a `bbox` that encompasses all the features within this GeoEntityData. The purpose of this is to facilitate efficient querying of whether the geo entity would be within a certain bounds (for the purposes of rendering or calculations).&quot; }, &quot;user_id&quot;: { &quot;description&quot;: &quot;The user this annotation is associated with&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;annotation_id&quot;, &quot;features&quot;, &quot;meta_geo&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Annotation&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Annotations are overlays placed on the map to convey useful information (such as marking out different zones for special purposes, or describing planned work etc.)&quot;, &quot;properties&quot;: { &quot;data&quot;: { &quot;$ref&quot;: &quot;#/definitions/AgisAnnotationData&quot;, &quot;description&quot;: &quot;Data properties are dependent on the entity type&quot; }, &quot;entity_type&quot;: { &quot;const&quot;: &quot;annotation&quot;, &quot;description&quot;: &quot;The type of the geoentity, which will affect the data's properties&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;id&quot;: { &quot;description&quot;: &quot;A uuid for the data (should be unique across all AOH GIS Geo Entities)&quot;, &quot;type&quot;: &quot;string&quot; }, &quot;version&quot;: { &quot;description&quot;: &quot;The semver of the geo entity schema that this object complies with&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;data&quot;, &quot;entity_type&quot;, &quot;id&quot;, &quot;version&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;AnnotationFeatureCollection&quot;: { &quot;$ref&quot;: &quot;#/definitions/FeatureCollection%3Cnull%2CAnnotationFeatureProperties%3E&quot; }, &quot;AnnotationFeatureProperties&quot;: { &quot;additionalProperties&quot;: false, &quot;type&quot;: &quot;object&quot; }, &quot;BBox&quot;: { &quot;anyOf&quot;: [ { &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;maxItems&quot;: 4, &quot;minItems&quot;: 4, &quot;type&quot;: &quot;array&quot; }, { &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;maxItems&quot;: 6, &quot;minItems&quot;: 6, &quot;type&quot;: &quot;array&quot; } ], &quot;description&quot;: &quot;Bounding box https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;Feature&lt;null,AnnotationFeatureProperties&gt;&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;A feature object which contains a geometry and associated properties. https://tools.ietf.org/html/rfc7946#section-3.2&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;geometry&quot;: { &quot;description&quot;: &quot;The feature's geometry&quot;, &quot;type&quot;: &quot;null&quot; }, &quot;id&quot;: { &quot;description&quot;: &quot;A value that uniquely identifies this feature in a https://tools.ietf.org/html/rfc7946#section-3.2.&quot;, &quot;type&quot;: [ &quot;string&quot;, &quot;number&quot; ] }, &quot;properties&quot;: { &quot;$ref&quot;: &quot;#/definitions/AnnotationFeatureProperties&quot;, &quot;description&quot;: &quot;Properties associated with this feature.&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Feature&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;geometry&quot;, &quot;properties&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;FeatureCollection&lt;null,AnnotationFeatureProperties&gt;&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;A collection of feature objects. https://tools.ietf.org/html/rfc7946#section-3.3&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;features&quot;: { &quot;items&quot;: { &quot;$ref&quot;: &quot;#/definitions/Feature%3Cnull%2CAnnotationFeatureProperties%3E&quot; }, &quot;type&quot;: &quot;array&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;FeatureCollection&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;features&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Point&quot;: { &quot;additionalProperties&quot;: false, &quot;description&quot;: &quot;Point geometry object. https://tools.ietf.org/html/rfc7946#section-3.1.2&quot;, &quot;properties&quot;: { &quot;bbox&quot;: { &quot;$ref&quot;: &quot;#/definitions/BBox&quot;, &quot;description&quot;: &quot;Bounding box of the coordinate range of the object's Geometries, Features, or Feature Collections. The value of the bbox member is an array of length 2*n where n is the number of dimensions represented in the contained geometries, with all axes of the most southwesterly point followed by all axes of the more northeasterly point. The axes order of a bbox follows the axes order of geometries. https://tools.ietf.org/html/rfc7946#section-5&quot; }, &quot;coordinates&quot;: { &quot;$ref&quot;: &quot;#/definitions/Position&quot; }, &quot;type&quot;: { &quot;const&quot;: &quot;Point&quot;, &quot;description&quot;: &quot;Specifies the type of GeoJSON object.&quot;, &quot;type&quot;: &quot;string&quot; } }, &quot;required&quot;: [ &quot;coordinates&quot;, &quot;type&quot; ], &quot;type&quot;: &quot;object&quot; }, &quot;Position&quot;: { &quot;description&quot;: &quot;A Position is an array of coordinates. https://tools.ietf.org/html/rfc7946#section-3.1.1 Array should contain between two and three elements. The previous GeoJSON specification allowed more elements (e.g., which could be used to represent M values), but the current specification only allows X, Y, and (optionally) Z to be defined.&quot;, &quot;items&quot;: { &quot;type&quot;: &quot;number&quot; }, &quot;type&quot;: &quot;array&quot; } } } ","keywords":"","version":"Next"},{"title":"Piechart with Progress bar","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/api/charts/piechartwithprogressbar","content":"Piechart with Progress bar This widget combines a pie chart and a progress bar, offering a comprehensive visual insight into data distribution and ratio representation Configuration Data​ Parameter\tType\tDescriptionTitle\tstring\tTitle of the widget Datasource\tstring\tTable name of the data to display Properties​ Parameter\tType\tDescription\tRequiredOrder\tString\tUnique identifier of the row\tYes Label\tString\tAn attribute that allows the rows to be grouped\tYes Count\tString\tThe column that shows the value tied to the Label attribute\tYes Group\tString\tThe column to determine whether the row is used for the dropdown or the piechart\tYes How to use Assuming this is the content of your datasource named aoh_charts_group_count_table and you wish to group the columns between calls. label\tcount\tgroup0-4H\t10\tNOT_DROPDOWN 20-24H\t23\tNOT_DROPDOWN 4-20H\t30\tNOT_DROPDOWN 30 days\t200\tIS_DROPDOWN 60 days\t362\tIS_DROPDOWN The widget contains 2 segments: The piechart and the dropdown menu on the footer. So for this example, the column group will be the one that is determining which row goes to which segment. note The data requires an attribute that determines whether the row goes into the piechart or the dropdown. 1. Specify the datasource​ Select the datasource that you wish to display in the datsource dropdown. 2. Configure the order key.​ Specify the column in the datasource that you wish to sort the order by. This will sort the the rows based on the column you specified. 3. Specify the label key​ Specify the column that will give you the labels for the pie charts and dropdown menus. For this example, the column to use for this datasource would be the label column If you want to group your rows based on a certain type, you may do so by specifying the column key that holds the type. This will create a dropdown menu on the header that allows you to display your rows based on what it is grouped by. (If you do not specify this value, the dropdown menu will not appear). 4. Choosing the value key​ Pick the column that has the value tied to the labels chosen above. For this example, the column count is used. 5. Grouping the rows​ As mentioned above, the rows will be grouped into 2 segments, one for the piechart and the other for the dropdown menu. So select the key in your table that has the indicator to determine which row goes to which. You then input the values in these field that will determine which row will go to which segment. For this example, we will use the group column and in the group column. The rows with the value of IS_DROPDOWN will go to the dropdown menu in the &quot;Total From Last section&quot; and those with the value of NOT_DROPDOWN will go into the pie chart. After that the widget will display on your dashboard with the given datasource.","keywords":"","version":"Next"},{"title":"Dashboarding","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/api/dashboard","content":"Dashboarding AGIL Ops Hub deals with dashboarding with a custom widget system using GridStack.js. Widgets can be moved, resized, added, and removed in a dynamic grid. Extending your components to work with the widget system​ To enable your component to be registered to the widget system, go to the add a folder called WidgetConfig and create anindex.svelte file in it. This will make your component appear on the widget-view route, where it can be added or removed from the widget dashboard. Your WidgetConfig component is meant to provide configuration controls for your new 'widgetized' component, here is an example configuration widget that just accepts a text input. The text input's value is then bound to the desired config property (in this case threshold), and that value gets passed to your widget in the widget-view. You must declare the 'config' prop and use it to pass values to your main widget. MyFavouriteComponent/WidgetConfig/index.svelte &lt;script lang=&quot;ts&quot;&gt; export let config = { color: &quot;red&quot; }; //default &lt;/script&gt; &lt;div&gt;Config: &lt;/div&gt; &lt;input bind:value={ config.color } /&gt; Inside your main component, to read the config data, you export the same 'config' prop - then read that prop and use the values you want accordingly. When your components are rendered dynamically, the 'config' prop is passed from the configuration component to the main component. This config can then be persisted (in a database or whatever... in our case, we are storing it in the database) and then read to load widget-specific configuration. MyFavouriteComponent/index.svelte &lt;script&gt; export let config = { color: &quot;red&quot; }; // 2-way bound default &lt;/script&gt; &lt;div style=&quot;border: 1px solid; padding: 1rem; ;{ config ? `color: ${config.color}` : ''}&quot;&gt; Foo Bar &lt;/div&gt; note If you want to know more, you can view this simple example on StackBlitz to understand how the components are dynamically rendered and the config prop is passed in: https://stackblitz.com/edit/dynamic-rendering-with-config The StackBlitz example is a heavily simplified version without GridStack and saving incorporated. Providing a default widget size, title, and icon​ Upon adding your widget to the widget view, it will get initialized with a default size - to ensure your widget looks nice, you should specify what that size should be, you do this by exporting some constants that will be bound to your widget when it's initialized in the grid. You will also need to export a title, and Font Awesome icon that will be used to represent your widget for during selection. MyFavouriteComponent/index.svelte &lt;script lang=&quot;ts&quot;&gt; import { faToilet } from '@fortawesome/pro-duotone-svg-icons'; export let config = { color: &quot;red&quot; }; // 2-way bound default export const widgetTitle = 'My Favourite Widget'; export const widgetIcon = faToilet; export const widgetWidth = 3; export const widgetHeight = 4; &lt;/script&gt; &lt;div style=&quot;border: 1px solid; padding: 1rem; ;{ config ? `color: ${config.color}` : ''}&quot;&gt; Foo Bar &lt;/div&gt; ","keywords":"","version":"Next"},{"title":"Theming System","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/api/theming","content":"Theming System Go straight to the implementation section if you want to know the details on how we handle our theming. Switching themes​ To switch themes programmatically, import 'theme' from the Theme.ts store in src/lib/stores/Themes.ts, and change it to whatever theme you want: import { theme, THEMES } from &quot;$lib/stores/Theme&quot;; $theme = THEMES.OPSHUB; This triggers a different .css file to be loaded, resulting in the theme change. Creating a new theme​ To create a new theme, add a new .css file under /static/themes/{new-theme-name}.css. /static/themes/{new-theme-name}.css .theme { /*Primary*/ --primary: 250 83 5; --primary-dark: 250 83 5; --on-primary: 255 255 255; --on-primary-dark: 255 255 255; --primary-container: 33 33 33; --primary-container-dark: 33 33 33; /** ... and much more **/ } This results in you overriding the .theme class with different values for the css variables. The values are in RGB, without commas - this gets read by a function by Tailwind - which allows us to add opacity values; this is so you get the ability to append a slash and a number when using these Tailwind classes to get opacity. The following example results in using the on-primary color, with 60% opacity, applied to texts (font color - basically CSS's color property). &lt;div class=&quot;text-on-primary/60&quot;&gt;Hello World!&lt;/div&gt; For your new theme to be coherent, you must thoughtfully choose all the colors for every CSS variable. You can find the complete list of CSS variables you need to define in the existing examples in the /static/themes folder, or in thetailwind.config.cjs file, at the theme.extend.colors setting. You must also extend the theme list in the Theme.ts store: /src/lib/stores/Themes.ts {new-theme-name} Extending themes​ To extend the theme with more colours to choose from (we use Material UI's specification as a baseline), you may go to the tailwind.config.cjs file and add more properties to the theme.extend.colors setting. Each property there corresponds to a color in Tailwind and these colors are mapped to your theme colours in the .css file you define through CSS variables. For example, if you need a new type of themeable colour called attention, you can add it like so: tailwind.config.cjs module.exports = { ..., theme: { ..., extend: { ..., colors: { ..., 'attention': withOpacityValue('--attention') } }, } } The function withOpacityValue is defined at the top of the file - it is used by Tailwind to handle alpha in colors. Understanding the implementation​ At the root layout in our Svelte Kit app, the theme store changes a reactive variable that decides which .css file gets loaded in our &lt;head&gt; element. src/routes/+layout.svelte &lt;script lang=&quot;ts&quot;&gt; ... onMount(() =&gt; { // Modify themeTouse when theme store changes theme.subscribe(changedTheme =&gt; { if (window) { let storedTheme = window.localStorage.getItem('theme') as THEMES; // Give priority to stored theme if (storedTheme) { themeToUse = storedTheme; } else { themeToUse = changedTheme; } } }); }); ... &lt;script&gt; ... &lt;svelte:head&gt; &lt;title&gt;{$title}&lt;/title&gt; &lt;!-- Load css based on theme name --&gt; &lt;link rel=&quot;stylesheet&quot; href={'/themes/' + themeToUse + '.css'} /&gt; &lt;/svelte:head&gt; ... The alternative is to change the CSS appended to the root element ( &lt;html&gt; ) based on the theme. Both approaches requires javascript to load preferences in the user's localStorage before updating to the final theme. With our approach we only get the flash of the different theme on the initial server-rendered page. We want to maintain fast rendering instead of hiding/pausing visible elements and only showing them the pageafter the final theme is chosen. The flash can be solved by the server also rendering the correct preference - either via storing preferences in the database or using cookies. The plan is to use a cookie-based solution for this. We also add support for the user to change themes by storing their preference on the device (localStorage API). You should allow the user to set or clear this local storage if you wish to allow them to change themes on-the-fly, on a per-device basis. If no stored theme is used, the app will default to whatever gets loaded in the latest layout. This way, you can switch between themes depending on the layout or page by setting the theme store.","keywords":"","version":"Next"},{"title":"Generic Table Widget","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/api/charts/table","content":"Generic Table Widget This widget provides a dynamic and interactive table that displays data in a user-specified format while offering filtering and column grouping functionalities. Configuration Data​ Parameter\tType\tDescriptionTitle\tstring\tTitle of the widget Datasource\tstring\tTable name of the data to display Properties​ Parameter\tType\tDescription\tRequiredID\tString\tUnique identifier of the row\tYes Category\tString\tAn attribute that allows the rows to be grouped\tNo Header Groups\tHeaderGroup[]\tLink multiple columns into a single group\tYes For HeaderGroup type, Please refer below. HeaderGroup type​ Members\tType\tDescriptionlabel\tString\tServes as the label or name for the header group. attr\tString\tDetermines a column key from the datasource under the given header group. How to use Assuming this is the content of your datasource named aoh_charts_group_count_table and you wish to group the columns between calls. unit\tdepartment\ttotal_calls\taverage_calls\topen_cases\tclosed_casesHQ SCDF\tSCDF\t100\t19\t200\t50 Old Police Academy\tSPF\t10\t200\t40\t120 1st SCDF Div Queensway Camp\tSCDF\t10\t5\t4\t3 1. Specify the datasource​ Select the datasource that you wish to display in the datsource dropdown 2. Configure the row identifier.​ Specify the column key of your table that identifies the row, This value will always be the first column on the table. 3. Specify the category (Optional)​ If you want to group your rows based on a certain type, you may do so by specifying the column key that holds the type. This will create a dropdown menu on the header that allows you to display your rows based on what it is grouped by. (If you do not specify this value, the dropdown menu will not appear). 4. Creating header groups and grouping columns​ You may then create the header groups and assign your datasource columns to the respective header group as you desire. Create the header group and give them a name.Create an entry under the headergroup and assign them the column key to be associated with that header groupYou may rename the column header by assigning a label beside it. (optional) After completing the following instructions, the table will be displayed.","keywords":"","version":"Next"},{"title":"Configuration","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/guides/configuration","content":"Configuration There are many levels of configuration available, however, to keep things simple, we'll only cover configuration specific to aoh-web. Refer to the list below for helpful information on configuring the other modules. The following list of environment variables are used during both development and deployment of the project. Environment Variables​ Environment variables are managed by using a .env file in the root of the project. This .env file is loaded by multiple programs and used in different ways. It is used by docker-compose to run the Dockerfile, and it is used by Vite / Svelte Kit in two ways: Static: Processed during build-time and statically replaced, these cannot change during run-time.Dynamic: These can be changed during run-time by modifying the environment variable. Refer to the Svelte Kit $env API to learn how to access these variables (code in Svelte Kit is differentiated as being available on both the client &amp; server or server only. Code that is exposed to the client can only import env vars that are marked PUBLIC_ (to avoid leaking sensitive information to the browser). The full list of environment variables are available in the .env.sample file If you need to add more variables to the .env file, remember to update the .env.sample and the docker-compose.ymlfile's environment property to document the newly required variable(s). Since Vite loads additional .env files based on the NODE_ENV (e.g. when NODE_ENV=development, it will load .env.development), you can load and override variables using a .env.[mode] file. info Running npm run dev will run node with NODE_ENV=development. See Vite's official documentation. Setting your npm configuration to use the tokens​ For local development, we recommend that you set the access tokens by running the npm set command: npm set //npm.fontawesome.com/:_authToken=&lt;YOUR_FORTAWESOME_ACCESS_TOKEN&gt; npm set //npm.pkg.github.com/:_authToken=&lt;YOUR_GITHUB_PERSONAL_ACCESS_TOKEN&gt; If you are running via Docker, you will need to set the FORTAWESOME_ACCESS_TOKEN and GITHUB_ACCESS_TOKEN env vars instead. Windows (PowerShell)Windows (cmd)Linux $Env:FORTAWESOME_ACCESS_TOKEN=&lt;YOUR_FORTAWESOME_ACCESS_TOKEN&gt; $Env:GITHUB_ACCESS_TOKEN=&lt;YOUR_GITHUB_PERSONAL_ACCESS_TOKEN&gt; info Visit the aoh-web wiki to retrieve the access token. caution Note that PORT is only used in production (when running node ./build/index.js). To run with a different port during development, use the --port flag: npm run preview --port 3001 Quality of Life - VS Code​ Format On Save​ We highly recommend you enable Format On Save. You can press Ctrl + P and type Open User Settings to navigate to the user settings page, then type format on save and the relevant setting will show up: Recommended¹ Extensions​ ¹ More of a requirement really... These are some extremely helpful extensions for Visual Studio Code and are integral to the development experience. Install the Official Extensions: Svelte for VS Code The Svelte extension provides code colouring and IntelliSense for Svelte, which itself is similar to jsx/tsx.Tailwind CSS IntelliSense The Tailwind CSS IntelliSense extension provides IntelliSense for Tailwind CSS.Playwright Test for VSCode The Playwright Test extension allows you to debug Playwright Tests, record new tests, tune selectors, run individual tests, and more.Prettier - Code formatterPrettier handles formatting - our files should all be automatically formatted the same way - in the future, we might integrate this into CI/CD.VS Code ESLint extension The ESLint extension will provide ESLint information as you code - when linting is enforced in the CI, you will have to adhere to the standards before your code is merged - this extension will help with that.Docker The Docker extension provides Docker functionality in VS Code, including and especially IntelliSense for Dockerfiles.GitLens The GitLens extension provides enhanced Git functionality in VS Code, including showing who committed which lines of code (no more comments saying &quot;//ADDED BY SO-AND-SO&quot;). Tailwind IntelliSense Hinters​ You can get IntelliSense for Tailwind CSS classes in VS Code with some extra configuration: //TODO Insert Screen2Gif instructions We use /* @tw */ comments to indicate that the following line of code is a Tailwind CSS class. To enable that configuration, add the following line to your VS Code settings. After which, adding / @tw / above variables you declare will have Tailwind IntelliSense. &quot;tailwindCSS.experimental.classRegex&quot;: [ &quot;/\\\\*\\\\s*@tw\\\\s*\\\\*/[^\\&quot;]+\\&quot;([^\\&quot;]*)\\&quot;&quot;, &quot;/\\\\*\\\\s*@tw\\\\s*\\\\*/[^']+'([^']*)'&quot;, ] note You can modify the regex to fit your own needs, but we recommend standardizing it so it works for all of us. Tailwind IntelliSense on Custom Attributes​ You should also configure the Tailwind VS Code extension's Class Attributes: Customizing this setting will allow custom component props to have Tailwind IntelliSense. Other Modules​ Complete configuration of all the modules is beyond the scope of this document. To configure the list of other modules, refer to their official documentation below: SvelteKit - https://kit.svelte.dev/docs/configuration SvelteKit can be configured in the svelte.config.js file in the root folder. SvelteKit Adapters - https://kit.svelte.dev/docs/adapters We use the @sveltejs/adapter-node adapter to create a simple node server. Vite - https://vitejs.dev/config/ Vite can be configured in the vite.config.js folder. Note that this used to be forwarded from the SvelteKit configuration. The Svelte maintainers have since changed this since the upgrade to Vite 3.0.0 Vitest - https://vitest.dev/config/ Vitest uses Vite's configuration, but you may override it by providing a vitest.config.js file in the root folder and passing --config vitest.config.test when running vitest. Playwright - https://playwright.dev/docs/test-configuration Playwright can be configured, in the playwright.config.ts file. GraphQL Code Generator - https://www.graphql-code-generator.com/docs/config-reference/codegen-config We use GraphQL Code Generator to generate the types from our GraphQL Schema. The configuration is in thecodegen.yml file. Our default configuration is to expect your schema.graphql file in the root folder. eslint - https://eslint.org/docs/latest/user-guide/configuring/rulesEslint is to help identify potential problems with our code and (along with our code formatter) help enforce our coding standards. TypeScript - https://www.typescriptlang.org/docs/handbook/tsconfig-json.htmlTypeScript can be configured in the tsconfig.json file in the root folder. Global types can be found in the ./src/global.d.ts folder. Postcss - https://github.com/postcss/postcss#usage Postcss is a peer dependency for Tailwind CSS. We currently do not configure it but it is necessary. npmrc - https://docs.npmjs.com/cli/v8/configuring-npm/npmrc The .npmrc file is used to configure the npm registries that we need such as GitHub Packages and FontAwesome.","keywords":"","version":"Next"},{"title":"Deployment","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/guides/deployment","content":"Deployment Production Preview​ Before preparing a build for production, ensure that you've conducted the appropriate tests. Build npm run build Run Preview (use production build files) npm run preview Deployment​ This project is deployed automatically via GitHub Actions and ArgoCD on every main branch commit. Refer to the .github/workflows yaml files to find out more about the workflows. Our pipeline for continuous deployment is: A pull request is submitted to merge a feature branch in.Unit tests are run on the pull requestWhen the pull request is merged, the integration tests are run and the files are deployed to the staging server. If you wish to deploy it yourself, run npm run build to generate the build folder and copy the build output to your servers. Then, run node ./build/index.js to start the web server. Windows (PowerShell)Windows (cmd)Linux npm run build $env:PORT=&lt;desired_port&gt;; node ./build/index.js Adding Environment Variables​ When you create new environment variables to use in the project, you will need to ensure that the CI/CD pipeline is configured with the right values for those variables and the container is able to read it upon deployment. There are 4 places to look out for: .env.sampledocker-compose.yamlDockerfile.github\\workflows\\web.yml Update the .env.sample​ Place your new environment variable defaults there for others to copy. Also, provide a short description on what it's for in the comments in the. Configure Docker Compose files​ In the docker-compose.yml file, you will need add additional args for the Dockerfile to take in docker-compose.yml services: web: build: args: - PUBLIC_FOO_BAR=${PUBLIC_FOO_BAR} Configure Dockerfile​ You will need to modify the Dockerfile accordingly to take in your new environment variable/s. Dockerfile ... ARG PUBLIC_FOO_BAR ENV PUBLIC_FOO_BAR=${PUBLIC_FOO_BAR} ... Configuring the CI/CD​ The CI needs to build your container image before being able to deploy it - depending on whether your environment variable is statically replaced upon building, or dynamically introduced based on the environment, you'll have to configure those environments accordingly. If your environment variable value is a secret, and you need it during build time, create asecret in GitHub. Also, make sure your GitHub workflow file has it available the container build step: .github\\workflows\\web.yml --- export PUBLIC_MAPBOX_KEY=${{ secrets.PUBLIC_MAPBOX_KEY }} ... jobs build: steps: - name: build container image run: | ... export PUBLIC_FOO_BAR=&quot;Hello There!&quot; ... docker-compose -f docker-compose.yml build web ","keywords":"","version":"Next"},{"title":"Documentation","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/guides/documentation","content":"Documentation JSDoc/TSDoc​ JSDoc is an API documentation generator for JavaScript - it's a widely popular tool that even has built-in support in Visual Studio Code. JSDoc overlaps with Typescript; Visual Studio Code or whatever IDE you're using that supports both JSDoc and TypeScript will be able to generate the appropriate hints using either system. TypeScript currently provides type information, which allows our IDE's to process and provide additional features during development, however, it's lacking in document comments. By using the JSDoc standard when writing our comments, we allow IDE's and their plugins/tools to parse and provide additional features. Essentially, we should follow the TSDoc standard (which is in a sense a rigorously specified version of JSDoc). .../src/lib/utils.ts /** * A function for Svelte actions - dispatches 'outclick' when a click happens outside the element * @param node This is passed to us via the Svelte Actions API, it refers to the Element this action being called for * * @example * import { clickOutside } from '$lib/utils'; * &lt;div use:clickOutside on:outclick={() =&gt; { alert(&quot;You clicked outside of the div!&quot;)} /&gt; */ The following is a screenshot of the VS Code output when you hover on the imported function, commented with the above screenshot. Svelte has its own format for documenting components with the @component hint: .../Button/index.svelte &lt;!-- @component Button can take on:click callbacks. The Button component also accepts a slot for a child element. Props: - `variant`: The variant of the button, controls the default styles. - `appendClass`: Pass in a class string as `appendClass` to append to the built-in component styles. - `setDisabled`: Pass in false to disable the button. - `setClass`: Pass in a class string to override the default built-in component styles. - `setStyle`: Pass in a styles to override styles and classes (higher specificity than TailWind) - Usage: ```tsx &lt;script lang=&quot;ts&quot;&gt; import Button from '$components/input/basic/Button/index.svelte'; &lt;/script&gt; &lt;Button setDisabled={ true }&gt;Push My Button&lt;/Button&gt; ``` --&gt; The following is a screenshot of the VS Code output when you hover on the Button svelte component, commented with the above screenshot. Keeping your components and supporting libraries/pages up to date with components is incredibly helpful for new developers taking over your code, or even yourself after you revisit your code in the future. Docusaurus​ We use Docusaurus to create this documentation website. The Docusaurus site is automatically published from the ./docusaurus directory (it is a separate React website) when code is pushed. Edit the .md or.mdx files to make changes. When browsing the documentation, you may click the 'Edit this page' button to edit the markdown file directly on GitHub. Saving it will trigger the documentation site to be re-built automatically. // TODO: API Documentation using OpenAPI - converting to Docusaurus API","keywords":"","version":"Next"},{"title":"Quickstart","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/guides/quickstart","content":"Quickstart Pre-requisites​ Please install the following programs and tools before setting up aoh-web: Recommended Tools: Visual Studio CodeDocker Desktop Required: node.js (version ^18.12.1)git Running Locally​ Clone the repository: git clone https://github.com/mssfoobar/aoh-web caution If you are not contributing directly to aoh-web, use https://github.com/mssfoobar-rnd/aoh-web instead. You should also fork the project instead if you are using it as a template. Font Awesome requires an access token to install their modules from their private registry. You will also need to set up your personal GitHub Access Token to access our GitHub Packages repository. Set up the access tokens: npm set //npm.fontawesome.com/:_authToken=&lt;YOUR_FORTAWESOME_ACCESS_TOKEN&gt; npm set //npm.pkg.github.com/:_authToken=&lt;YOUR_GITHUB_PERSONAL_ACCESS_TOKEN&gt; info Sensitive tokens are not stored online, you'll need to ask your project manager for help with getting the access tokens. Copy the sample environment variable file and name it .env cp .env.sample .env This .env file will have its values loaded by Vite (as well as docker-compose) upon building and is made available via the $env API in Svelte Kit. Install the node modules npm install Get the latest GraphQL schema npm run getschema danger There is a 'predev' npm script that attempts to re-generate our GraphQL types everytime you run npm run dev. You must ensure your .env file has the GRAPHQL_ENDPOINT variable set for npm run getschema to pull the schema from the correct endpoint (seethe development guide on getting types for GraphQL queries) If you do not have a GraphQL endpoint set up and wish to omit the type generation, you must delete the predev script from the package.json file to prevent the graphql-codegen script from running. Generate the .svelte-kit folder npx svelte-kit sync This command is also run as a pre-build step - it generates types from your code, including anything that's loaded in the .env files, allowing you to have Intellisense for the environment variables and more good stuff. Run the server in development mode npm run dev Visit http://localhost:5173 to view the website Docker​ For development purposes, you should run the project locally. However, to run it with Docker, ensure you haveDocker Desktop installed. Before running the container, you must provide the access tokens for docker-compose to read. In step 2, you would set it up with npm config, but for our Docker setup, you must pass them as environment variables: FORTAWESOME_ACCESS_TOKENGITHUB_ACCESS_TOKEN You may do so by providing a .env.local file: FORTAWESOME_ACCESS_TOKEN=&lt;YOUR_FORTAWESOME_ACCESS_TOKEN&gt; GITHUB_ACCESS_TOKEN=&lt;YOUR_GITHUB_PERSONAL_ACCESS_TOKEN&gt; caution It is FORTAWESOME with an 'R', not FONTAWESOME. Previous versions of AGIL Ops Hub used a FONTAWESOME token, this has changed as we now host the packages ourselves for our CI to install. If you have a Fontawesome license and access to the official Fortawesome private NPM registry, you should use that token and set up your CI with that instead. You may also supply PORT to override the default port. FORTAWESOME_ACCESS_TOKEN=&lt;YOUR_FORAWESOME_ACCESS_TOKEN&gt; GITHUB_ACCESS_TOKEN=&lt;YOUR_GITHUB_PERSONAL_ACCESS_TOKEN&gt; # Example to run on port 3001 PORT=3001 danger .env is git ignored in our repo, do no put your tokens into the .env file; do not check your tokens into source! Then, you can run the container with: docker-compose up The docker-compose also uses 2 more environment variables that are required during build time: IMAGE_TAGPUBLIC_STATIC_BUILD_VERSION These are to be set by the CI to allow us help with troubleshooting in deployed environments. For local development, they are not necessary. note PUBLIC_STATIC_BUILD_VERSION is used in the ar2's info endpoint to provide the build version. See the [API docs] for more information. You may view the Dockerfile and associated docker-compose for more details. Using the aoh-web components as a dependency​ You can also use the published components by installing @mssfoobar/aoh-web and all it's peer dependencies. npm install @mssfoobar/aoh-web tailwindcss postcss svelte @sveltejs/kit note aoh-web components are built for use in SvelteKit projects only, and uses Tailwind CSS. You must Svelte and SvelteKit installed and properly set up.","keywords":"","version":"Next"},{"title":"Publishing","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/guides/publishing","content":"Publishing Manual Publishing​ 1. Test and use the packages before publishing:​ In order to import components from @mssfoobar/aoh-web for testing, use npm link(see https://docs.npmjs.com/cli/v8/commands/npm-link)This allows us to use components from @mssfoobar/aoh-web without adding it as a dependency in package.json. We can then test against our local copy before actually publishing them to the npm registry. cd package npm link cd ../ npm link @mssfoobar/aoh-web 2. Publishing the ready packages:​ The Svelte components here are published to @mssfoobar/aoh-web, to package and publish the components, perform the following steps: First, run svelte-kit package (experimental as of 10th May 2022) to create a package of the files in lib/src. This may be configured via the svelte.config.js file. See: https://kit.svelte.dev/docs/configuration#package &amp; https://kit.svelte.dev/docs/packaging npm run package Then, publish the packaged file. This repository defaults to publishing to our private registry, configured in the .npmrc file for installing npm modules, and the package.json file (publishConfig) for publishing @mssfoobar/aoh-web. cd package npm publish note You will be required to log in to access the private repository, obtain an account from the repository maintainer. Automatic Publishing​ GitHub Actions has been configured to automatically publish the package when a new release is created. For more information on how to publish releases on GitHub, visit https://docs.github.com/en/repositories/releasing-projects-on-github/managing-releases-in-a-repository.","keywords":"","version":"Next"},{"title":"BDD Testing","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/guides/testing/BDD-Testing","content":"BDD Testing Behavior-Driven Development (BDD) Testing is a powerful and user-centric approach to software testing that emphasizes collaboration between developers, testers, and stakeholders. Introduction​ We use Cucumber framework that facilitates Behavior-Driven Development (BDD). It allows you to express test scenarios in a simple text format (Given-When-Then format) and then link them to executable code. When running the Cucumber tests, it essentially associates the feature file with its corresponding step definition and executes the code therein. You have the flexibility to employ any assertion framework, to handle assertions within the code. Here we are using Playwright for assertions. Installation​ Navigate to the folder after cloning and install npm packages. npm install To download the necessary browsers for Playwright during the initial installation, execute the following command. npx playwright install Running the tests​ To run the test cases, execute the following command. npm run test To run only the failed test cases after the initial execution, use the following command. npm run test:failed Configuration​ The details of the configuration can be found here Allure Reporting Tool​ Allure is an open-source test reporting tool that provides visually appealing and comprehensive reports for test results. After executing the test cases, the reports are automatically generated, eliminating the need for a separate command to run them.","keywords":"","version":"Next"},{"title":"🆕 Introduction","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/introduction","content":"🆕 Introduction The web application of AGIL Ops Hub ties all the different modules together. As such, it can be thought of as the biggest service in the system. Each module in the system may have zero or more back-end services supporting it. As the web is the front of the entire system, it essentially depends on all other services to function properly. Setting up the other services is beyond the scope of this document; you should refer to their approriate documentation on how to set them up there, but we will point out whenever you need a specific service to run a specific feature. Visit the technologies section for more information. TODO: - Incorporate Typescript checks // Playwright tests before or as part of CI/CD ![status](https://github.com/mssfoobar/aoh-web/actions/workflows/web.yml/badge.svg) - To standardize formatting, we are using `prettier-plugin-tailwindcss`, which also handles Svelte code. - We plan to have `eslint` strictly lint the project (configuration requires review). ","keywords":"","version":"Next"},{"title":"TDD Testing","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/guides/testing/TDD-Testing","content":"TDD Testing Test-Driven Development (TDD) is a software development approach that emphasizes writing tests before writing the actual code. Introduction​ We use Vitest for unit testing and Playwright for end-to-end tests. We use Vitest becacuse it is Vite-native (this project is using Svelte Kit which uses Vite) and Jest compatible (Jest is currently one of the most popular JavaScript unit testing frameworks). We use Playwright because it is both the recommended end-to-end testing framework for Svelte Kit, and it is also very feature rich, performant, well maintained, and quickly growing in popularity. info Learn some of Playwright's important features: https://www.youtube.com/watch?v=PXTspGn1im0 Running the tests​ Ensure you install the browsers that Playwright uses: npx playwright install Execute the tests npm run test Tests are actually broken up into 3 parts, and running npm run test runs them all - unit tests, component tests, and end-to-end tests. We've broken things up in this way as we've found the best way to run tests on our individual Svelte components is to have them rendered out (instead of using something like jsdom to emulate the browser) - so we use Playwright to run unit tests on our components, and Vitest to run unit tests on scripts. If you wish to only run unit tests, run: npm run test:unit If you wish to only run component tests, run: npm run test:comp If you wish to only run integration tests, run: npm run test:e2e Code Coverage​ Vitest has native code coverage via c8, run the script with: npm run coverage This coverage is only for unit tests via Vitest. Developing the tests​ Unit Tests​ Unit tests are Vitest tests (see the Vitest Docs) for more info. They are placed side-by-side in the directory of the file they are testing and have the same name as the file, with either .spec added to the name. For example, tests for the ./src/lib/utils.ts file should be./src/lib/utils.spec.ts. Component Tests (unit tests for components)​ The best way to test our components is by running the full end-to-end tests, which requires Playwright(see the Playwright Docs) for more info. The component tests are meant be both an example of how to use the component, and the page that the tests run against. These tests comprise of 2 parts, the test.svelte file that instantiates and renders the component, and thecomp.test.ts file is run by Playwright to visit the test.svelte page and perform actions to test the component. Every test.svelte file should have a corresponding comp.test.ts file and a route that can be accessed by by Playwright - our folders are structured such that the src/routes/tests folder follows the folder structure of the components to be tested, so the routes are intuitive and can be navigated to programmartically. End-to-end or Integration Tests​ End-to-end tests are Playwright tests (see the Playwright Docs) for more info. They are placed in the ./tests-e2e directory. There is a default example.test.ts file that you can use to get started. Continuous Integration​ This project has CI configured via GitHub Actions, refer to the .github/workflows yaml files to find out more. Allure Reporting Tool​ Third party reporting tool for playwright test framework Installation​ Run the following command to install dependancies npm install allure-playwright View Report​ Run the following command to view report npm run view:report Usage of test ids for component identification​ Every component to be tested must include an additional attribute called data-testid. Please follow this naming convention so that the naming is unique: data-testid=&quot;&lt;component-name/page-name&gt;-&lt;component-functionality-name&gt;-&lt;ui/control-type&gt;&quot; Example Login button on a log in page &lt;button data-testid=&quot;login-login-bttn&quot; /&gt; For long componentFunctionalityName like passwordError follow &lt;h1 data-testid=&quot;login-password-error-lbl&quot;&gt;&lt;/h1&gt; For login button inside a modal it can be like this &lt;button data-testid=&quot;login-modal-login-bttn&quot; /&gt; ","keywords":"","version":"Next"},{"title":"💻 Development","type":0,"sectionRef":"#","url":"/aoh-docs/docs/wfe/guides/Workflow Worker/development","content":"💻 Development Activity​ An Activity is a normal function or method that executes a single, well-defined action (either short or long running), such as calling another service, transcoding a media file, or sending an email message. Activity code should be deterministic. Activity Function Definition​ Lets define a simple activity which has input and output as string data type. type Activities struct{} func (a *Activities) SimpleActivity(ctx context.Context, input []interface{}) (string, error) { str, ok := input[0].(string) if !ok { return nil, errors.New(&quot;invalid input&quot;) } return &quot;Result_&quot; + str, nil } The first parameter of an activity definition is a context. Context is used by Temporal to pass around Workflow Execution Context. The second parameter is an array of interface{}. We use an array of interface{} to support the Workflow Engine to interpret as many parameters as defined by the Activity Developer. It is important to always typecast the input parameter and return error if there is to avoid runtime errors. And then we return the result as string. Dynamic Activity Registration​ If you notice in earlier example, we have declared Activites struct type and defined the function with reciever as Actvities. Reason is we want to register the activity dynamically which allow us to add new activites in Workflow Worker without having to rebuild the Workflow Engine. Otherwise, Workflow Engine service will need to statically register the activites defined in Workflow Worker service. func main() { // ... yourWorker := worker.New(temporalClient, &quot;your-custom-task-queue-name&quot;, worker.Options{}) yourWorker.RegisterActivityWithOptions(&amp;yourapp.Activities{}) err = yourWorker.Run(worker.InterruptCh()) // ... } ","keywords":"","version":"Next"},{"title":"💻 Development","type":0,"sectionRef":"#","url":"/aoh-docs/docs/wfe/guides/Workflow Engine/development","content":"💻 Development DSL​ WFE interprets dsl genereted by Workflow Designer Example Workflow DSL { &quot;name&quot;: &quot;ExampleDSL&quot;, &quot;schema_json&quot;: { &quot;variables&quot;: { &quot;Input&quot;: 1, &quot;Form_FormJSON&quot;: &quot;{}&quot; }, &quot;specVersion&quot;: &quot;2.0&quot;, &quot;root&quot;: { &quot;sequence&quot;: { &quot;elements&quot;: [ { &quot;activity&quot;: { &quot;activityId&quot;: &quot;d29a7b91-9a13-426d-9ce4-ccacd2f0e844&quot;, &quot;name&quot;: &quot;Activity&quot;, &quot;type&quot;: &quot;SampleActivityNumber&quot;, &quot;arguments&quot;: [&quot;Input&quot;], &quot;result&quot;: &quot;Activity_result&quot; } }, { &quot;parallel&quot;: { &quot;branches&quot;: [ { &quot;switch&quot;: { &quot;input&quot;: &quot;Activity_result&quot;, &quot;cases&quot;: [ { &quot;conditional&quot;: { &quot;operator&quot;: &quot;EQ&quot;, &quot;value&quot;: 1, &quot;element&quot;: { &quot;activity&quot;: { &quot;activityId&quot;: &quot;4cab9800-9feb-48ac-8803-98718f76e4cd&quot;, &quot;name&quot;: &quot;Activity1&quot;, &quot;type&quot;: &quot;SampleActivity1&quot;, &quot;arguments&quot;: [], &quot;result&quot;: &quot;Activity1_result&quot; } } } }, { &quot;conditional&quot;: { &quot;operator&quot;: &quot;EQ&quot;, &quot;value&quot;: 2, &quot;element&quot;: { &quot;activity&quot;: { &quot;activityId&quot;: &quot;128668dc-09d0-4b96-8f5c-878350f8057c&quot;, &quot;name&quot;: &quot;Activity2&quot;, &quot;type&quot;: &quot;SampleActivity1&quot;, &quot;arguments&quot;: [], &quot;result&quot;: &quot;Activity2_result&quot; } } } } ] } }, { &quot;form&quot;: { &quot;formId&quot;: &quot;e31a5f78-3c24-4136-849d-e15133d2d233&quot;, &quot;name&quot;: &quot;Form&quot;, &quot;type&quot;: &quot;Checklist&quot;, &quot;arguments&quot;: [&quot;Form_FormJSON&quot;], &quot;result&quot;: &quot;Form_result&quot; } } ] } } ] } } } } Above example workflow dsl will trigger the execution of the workflow in following steps:- . ├── execute Activity └── parallel ├── form └── switch ├── case │ └── execute Activity1 └── case └── execute Activity2 Workflow Event History​ History events are generated at run time by Temporal server. Refer to their documentation for more details. We use the generated events to track the status of the workflow execution. To handle the form submission and manual recovery of the switch state, WFE implements following addtional events - FormTaskScheduledFormTaskStartedFormTaskCompletedFormTaskFailedFormTaskCanceledRecoverTaskScheduledRecoverTaskStartedRecoverTaskCompletedRecoverTaskFailedRecoverTaskCanceled","keywords":"","version":"Next"},{"title":"🚀 Quickstart","type":0,"sectionRef":"#","url":"/aoh-docs/docs/wfe/guides/quickstart","content":"🚀 Quickstart WFE comprises three modules WFE (Workflowflow Engine - interpreter service for bpmn dsl)WFW (Workflow Worker - worker service which performs activity tasks)WFM (Workflow Manager - http server for workflow managmement) Pre-requisites​ Required: Docker Running Locally​ Clone Repo git clone https://github.com/mssfoobar/ar2-wfe.git Create .env file in project root directory LOG_LEVEL= APP_PORT= TEMPORAL_HOST= TEMPORAL_PORT= TEMPORAL_NAMESPACE= TEMPORAL_TASKQUEU= TEMPORAL_WORKFLOWTYPE= HASURA_HOST= HASURA_PORT= GQL_ENDPOINT= HASURA_GRAPHQL_ADMIN_SECRET= UCS_URL_PROTOCOL= UCS_HOST= UCS_PORT= Start services by running command go run cmd/workflow-manager/main.go go run cmd/workflow-engine/main.go go run cmd/workflow-worker/main.go Docker setup​ Install DockerInstall Docker-ComposeDocker Post-installation setupClone git clone https://github.com/mssfoobar/ar2-wfe.git Start docker-compose docker-compose up -d Cleanup Docker docker-compose down ","keywords":"","version":"Next"},{"title":"Development","type":0,"sectionRef":"#","url":"/aoh-docs/docs/web/guides/development","content":"Development The scope of this document is not to be a tutorial on Svelte andSvelte Kit, you should visit their official documentation to learn more about them. This is meant to be a quick reference to help you can create components using our stack; how our folders are structured, how state is managed, how we approach unit and end-to-end testing, etc. Creating A Simple Svelte Component​ Our components rest inside the src/lib/components folder. Each component rests in a separate folder with the following files: index.svelte: The Svelte file for the componentindex.d.ts: Ambient declaration file to support the componenttest.svelte: An instantiation of the component, used to test the component, as well as provide examples on how to use itcomp.test.ts: The Playwright test for the component - the project is configured to visit and perform actions on the test.svelte file via the /tests route. index.svelte​ View the code on Svelte REPL The following is a simple Svelte component demonstrating data binding, reactivity, and iteratively rendering based on data. index.svelte &lt;script&gt; let exampleObjects = []; let currentText = &quot;Hello new developer!&quot;; let totalCount = 1; function addObject() { exampleObjects.push({text: currentText, count: totalCount}); exampleObjects = exampleObjects; // reactivity is based on assignments, assign it to itself to force reactivity totalCount++; } &lt;/script&gt; &lt;input type=&quot;text&quot; bind:value={currentText}&gt; &lt;button on:click={addObject}&gt; ADD OBJECT &lt;/button&gt; &lt;div&gt; {#each exampleObjects as xo } &lt;div&gt; {xo.count}. {xo.text} &lt;/div&gt; {/each} &lt;/div&gt; Though JavaScript is loved for it's speed of development and flexibility, the great freedom provided by it also makes maintaining large projects a nightmare. TypeScript is a must - it allows yourself and other developers to understand the shape of the objects being passed around. To have your types available throughout the project, we use an ambient typescript declaration file: index.d.ts​ Here's an example declaration for the component we wrote: index.d.ts declare interface ExampleObject { text: string; count: number; } And here is the TypeScript version of that componet (the index.svelte) shown earlier. index.svelte &lt;script lang=&quot;ts&quot;&gt; let exampleObjects: Array&lt;ExampleObject&gt; = []; let currentText = &quot;Hello new developer!&quot;; let totalCount = 1; function addObject() { exampleObjects.push({text: currentText, count: totalCount}); exampleObjects = exampleObjects; totalCount++; } &lt;/script&gt; &lt;input type=&quot;text&quot; bind:value={currentText}&gt; &lt;button on:click={addObject}&gt; ADD OBJECT &lt;/button&gt; &lt;div&gt; {#each exampleObjects as xo } &lt;div&gt; {xo.count}. {xo.text} &lt;/div&gt; {/each} &lt;/div&gt; With the ambient declaration file, we can use the ExampleObject throughout our project, enabling great features like type checking and IntelliSense. test.svelte​ Every component should have examples on how to use them, and tests to ensure they are running correct. We provide these by having a test.svelte file next to our index.svelte file. Example usage of index.svelte: test.svelte &lt;script&gt; import DeveloperGreeter from './index.svelte'; &lt;/script&gt; &lt;div&gt; &lt;h1&gt; Look at this thing I made: &lt;/h1&gt; &lt;DeveloperGreeter /&gt; &lt;/div&gt; comp.test.ts​ For unit-testing components, we have the component folder structure mirrored in the /tests route, and we dynamically render the components based on the route. We then have Playwright visit the component pages to perform tests on them. Here is a sample of what the comp.test.ts file might look like for the component we wrote above: comp.test.ts import { test } from &quot;@playwright/test&quot;; test(&quot;click buttons&quot;, async ({ page }) =&gt; { // Go to http://localhost:4173/tests/components/basic/Button await page.goto(&quot;http://localhost:4173/tests/components/DeveloperGreeter&quot;); await page.locator(&quot;text='ADD OBJECT'&quot;).click(); await page.locator(&quot;text='1. Hello new developer!'&quot;).toBeVisible(); }); In the future, we might switch to Playwright's components feature. However, it is still currently experiemental. Styling and Theming​ Tailwind CSS is a responsive, highly extensible, utility-first CSS framework. It has a design system built-in, like Bootstrap, but unlike Bootstrap, it is much more flexible and meant to be used a little bit like in-lining your CSS. Read their core concepts to learn more about the rationale behind Tailwind. For theming, we use Tailwind's theme configuration system to create our own themes. In the tailwind.config.cjs file, you can see how we have it extended. The colors are named and assigned to a list of CSS variables that are assigned to the root element (html) - you can see this in the app.html file: app.html &lt;html lang=&quot;en&quot; class=&quot;theme-1 bg-background&quot;&gt;&lt;/html&gt; The default theme is called theme-1 and bg-background is the background color (specified with bg-) using thebackground key that we've assigned in the tailwind.config.cjs file. That background is given the CSS variable--background-color: 255 255 255, which is defined in themes.css. themes.css --background-color: 255 255 255; Changing this variable changes the color for anything using background and that is how we change themes on the fly. To define more themes, simply add more classes to the themes.css file and change the class on the html element. Example: themes.cssapp.html themes.css ... { /* other themes... */ } .theme-2 { /* Primary */ --primary-color: 62 78 111; --on-primary-color: 222 222 222; --primary-light-color: 124 124 255; --primary-dark-color: 12 13 80; /* Secondary */ --secondary-color: 152 170 32; --on-secondary-color: 12 12 12; /* and so on... */ } ... { /* other themes... */ } Pulling data with GraphQL​ Our framework uses GraphQL to pull data from the server, and GraphQL Subscriptions to receive updates. Making a GraphQL query​ In your index.svelte or more likely, your +page.ts or +page.server.ts file (please read up onrouting in Svelte Kit), you can make a GraphQL query using the gql tag. The graphql-tag package provides this ability to convert the JavaScript template literal into a GraphQL AST, which is then used by graphql-codegen to generate types for us to safely use. Example usage: gql` query WidgetConfig { configuration_widget { config name type uuid } } `; We store our GraphQL client (urql) in a Svelte Store, which you can get, and use to make queries via urql's API. danger To effectly understand the next section, you need a basic understanding of: Svelte &amp; Svelte KitGraph QLurql Here is an example Svelte page with a Svelte Kitload function that makes a GraphQL query and passes the result as props to the page component: The WidgetConfigQuery type and WidgetConfigDocument exports are generated using graphql-codegen, which will be elaborated on in the next section. +page.ts import { gqlClient } from &quot;$lib/stores/Clients&quot;; import { get } from &quot;svelte/store&quot;; import { type WidgetConfigQuery, WidgetConfigDocument, } from &quot;./index.generated&quot;; gql` query WidgetConfig { configuration_widget { config name type uuid } } `; export async function load({}) { const client = get(gqlClient); const response = await client .query&lt;WidgetConfigQuery&gt;(WidgetConfigDocument) .toPromise(); return { props: { configuration_widget: response.data?.configuration_widget, }, }; } +page.svelte &lt;script lang=&quot;ts&quot;&gt;... export let configuration_widget = {}; ...&lt;/script&gt; Making a GraphQL Subscriptions​ Urql performs GraphQL subscriptions using Wonka, a stream library. To handle data received from Urql subscriptions, they must be piped into a Wonka Subscribe function and handled with a callback function. The following is a simple example of setting up a subscription using our framework. See the comments for more details. +page.svelte &lt;script lang=&quot;ts&quot;&gt; import { SystemTimeDocument, type SystemTimeSubscription } from '$generated-types'; import { get } from 'svelte/store'; import { type Subscription, pipe as wPipe, subscribe as wSubscribe } from 'wonka'; import { gqlClientStore } from '$lib/stores/Clients'; import gql from 'graphql-tag'; import { onDestroy, onMount } from 'svelte'; let dateTime: ISO8601Date; let subscription: Subscription; onMount(async () =&gt; { // Get the GraphQL Client from the client store. const client = get(gqlClientStore); // Define the GraphQL subscription - this can be done in the same file, in a separate `.graphql` file, or // anywhere else - as long as it exists, graphql-codegen will pick it up and generate the subscription type and // object for use. Remember to run GraphQL Codegen! gql` subscription SystemTime { system_time { reported_at } } `; // Store the subscription so we can call unsubscribe later. subscription = wPipe( client.subscription&lt;SystemTimeSubscription&gt;(SystemTimeDocument, {}), wSubscribe(result =&gt; { if (result?.data &amp;&amp; result?.data?.system_time[0]) { dateTime = result.data.system_time[0].reported_at; console.log(&quot;Lets see the time!&quot;, dateTime); } }) ); }); onDestroy(async () =&gt; { // Ensure you unsubscribe at the appropriate time. For example, when the component unmounts. // If you skip this step, your subscription will persist as your change pages (with client-side navigation) if (subscription) subscription.unsubscribe(); }); &lt;/script&gt; Query Types &amp; GraphQL Codegen​ We use graphql-codegen to generate the query shape/types from the the schema.graphql file. You can get the schema file by using the introspection API from the GraphQL endpoint you are calling. In aoh-web, we have an npm script called getschema which runs gq to get the schema.graphql file and places It in the root directory. Learn how to use gq to pull the schema here:https://hasura.io/docs/latest/guides/export-graphql-schema/ npm run getschema We then have another npm script called generate which runs graphql-codegen. It reads the codegen.yml configuration file to determine how where to get schema.graphql (we place it in the root folder), along with all other files that store your queries and uses these files to generate the types. They appear in a folder called generatedwhich is excluded from .git. npm run generate After generating these types, you can now have full typescript definitions for your GraphQL queries. Remember to runnpm run getschema again whenever your GraphQL schema changes, and to run npm run generate whenever you write new queries, or need to update the queries with the new schema. Handling Authentication​ JWT Access Token​ Authentication of requests is done via the jwt. The urql client has been set up to use the JWT acquired from logging in to the system. It also automatically handles token refreshes and query retries. On the server-side client, the token is attached to the request header as a Bearer Authorization token. On the client side, it is sent to Hasuraautomatically via cookies. This usage of cookies on the client-side is an additional security measure to avoid exposing the token to access via JavaScript. This means you will not able to access the access token on the client/browser in JavaScript. To accomodate this, we have multiple instances of Hasura set up, each set to handle different modes of authentication. Broken Queries, Subscriptions, and Retries​ The urql client has been set up to handle retries via the retryExchange. The client is set up to retry only subscriptions, as it is assumed that when you make a subscription, you want this connection to be open and receiving data indefinitely. Queries are not retried, but you may configure this behaviour by modifying Clients.ts insrc/lib/stores/Clients.ts. Subscriptions use the graphql-ws transport, this creates and manages a websocket connection, which urql then uses. The JWT is passed to Hasura when the subscription is first created and the websocket connection gets opened. Hasura then uses the expiry time in the token to decide when to terminate the GraphQL connection. This closing of the connection causes all subscriptions to fail - then will then all be automatically retried with the new access token. In some cases, you might have passed a variable into your subscription that might need to be updated when a retry occurs. This is particularly necessary when you use Hasura's streaming subscriptions, which requires a cursor to determine where to start pulling data from. Your subscription would be retried with the original cursor value, causing your application to retrieve duplicate data. To deal with this, you can pass a context object with the propertyupdateOperationOnRetry to your GraphQL subscriptions. This must contain a callback function that takes in an operation as an argument and returns nothing. The GraphQL client will run this function on retry. Example: onMount(async () =&gt; { const client = get(gqlClientStore); gql` subscription ExampleStreaming($createdAt: timestampz) { some_data_stream { id } } `; subscription = wPipe( client.subscription&lt;ExampleStreamingSubscription&gt;( ExampleStreamingDocument, { created_at: dayjs().toISOString(), }, { // Pass in the callback function to update the operation variables on retry. updateOperationOnRetry: (operation: Operation) =&gt; { operation.variables = { created_at: dayjs().toISOString(), }; }, } ), wSubscribe((result) =&gt; { if (result?.data &amp;&amp; result?.data?.some_data_stream[0]) { logger.debug( { id: result.data.some_data_stream[0].id }, &quot;How? Is this what you want?&quot; ); } }) ); }); Managing State​ A key point to take note about using Svelte Stores with Svlete Kit is that whenever stores are run on the server, it is global - meaning when running hybrid rendering or server-side rendering (any code in .svelte that has noif (browser) checks or isn't run inside onMount()). This means you should always set stores in a browser context only. Reference issue: https://github.com/sveltejs/kit/discussions/4339 FontAwesome​ We are using the SVG+JS for extra features. combining this with SVG Sprites can allow us to save a lot of space. However, for maintainbility reasons, we are currently using the full set of icons (15MB). https://fontawesome.com/v6/docs/apis/javascript/tree-shaking Using tree-shaking, we can get rid of all the unused icons - this configuration will be handled in the future. https://fontawesome.com/docs/web/dig-deeper/webfont-vs-svg#side-by-side-comparison If many instances of the same icon is needed - use SVG Sprites for better performance. If many different icons are needed - load everything and use the Web Fonts + CSS method. The SVG + JS method of using Font Awesome is all contained in a js file (all.js). This can be massively optimized by choosing only the styles that you need. FontAwesome SVG+JS Testing​ Successful development involves testing. It is a big topic on its own, so please visit the respective pages to learn how we handle testing. Test Driven Development Testing (TDD)Behaviour Driven Development Testing (BDD) We are expected to write out own unit tests and participate in integration tests. Creating Endpoints​ All API endpoints created by the web server should be done at the src/routes/api folder. This is done by Creating+server.ts files in the respective folder. For example, for the api to get the system time, the +server.ts file is in src/routes/api/info/time/+server.ts. See https://kit.svelte.dev/docs/routing#server for more information; Standard endpoint message format​ As a standard for our system, we are adopting the following structure for the response payload for endpoints. This type is defined in the app.d.ts as HTTPResponseBody and should be used to type all your API responses. { data: {...}, // arbitrary format - always array? GraphQL does array or obj - to be decided message: &quot;...&quot;, // string sent_at: &quot;&quot;, //iso8601 errors: [ { message: &quot;....&quot;, // string ... } ] } ","keywords":"","version":"Next"},{"title":"Incident Management System","type":0,"sectionRef":"#","url":"/aoh-docs/docs/wfe/Incident Management System API/incident-management-system","content":"Version: 1.0.0 Incident Management System API for Incident Management System","keywords":"","version":"Next"},{"title":"🆕 Introduction","type":0,"sectionRef":"#","url":"/aoh-docs/docs/wfe/introduction","content":"🆕 Introduction WFE (Workflow Engine) is a eSOP system developed for AOH projects. At its core, WFE uses Temporal (workflow orchestration platform) to orchestrate business process model as per BPMN 2.0 spec. For more details about Temporal and BPMN 2.0, please refer to their official documentations https://temporal.io/https://www.omg.org/spec/BPMN/2.0/About-BPMN","keywords":"","version":"Next"},{"title":"Workflow Management","type":0,"sectionRef":"#","url":"/aoh-docs/docs/wfe/Workflow API/workflow-management","content":"Version: 1.0.0 Workflow Management API for workflow managment","keywords":"","version":"Next"}],"options":{"id":"default"}}